{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://qiita.com/memakura/items/20a02161fa7e18d8a693\n",
    "#https://qiita.com/y__ueda/items/7b6f2a95ea45667e1029\n",
    "\n",
    "def top_10_sites(vocab_list):\n",
    "    import re\n",
    "    import requests\n",
    "    import time\n",
    "\n",
    "    from bs4 import BeautifulSoup\n",
    "    from random import random\n",
    "    from selenium import webdriver\n",
    "\n",
    "    driver = webdriver.Chrome()\n",
    "    just_a_sec = random() \n",
    "\n",
    "    #vocab_list = [\"浅草\", \"デートスポット\", \"雑貨\"]\n",
    "    search_string = \"\"\n",
    "    for i in vocab_list:\n",
    "        search_string = search_string + i + '\"　\"'\n",
    "\n",
    "    driver.get('https://www.google.com/')\n",
    "    time.sleep(just_a_sec * 1)\n",
    "    search_box = driver.find_element_by_name(\"q\")\n",
    "    search_box.send_keys('\"' + search_string + '\"')\n",
    "    time.sleep(just_a_sec*1)\n",
    "    search_box.submit()\n",
    "    time.sleep(just_a_sec*1)\n",
    "\n",
    "    #for h1 in driver.find_elements_by_tag_name(\"h1\"):\n",
    "    #    print(h1.text)\n",
    "\n",
    "    #i = 0\n",
    "    url_list = []\n",
    "    XPATH = '//*[@id=\"rso\"]/div/div/div[\" + str(i + 1) + \"]/div/div/div[1]/a'\n",
    "    for a in driver.find_elements_by_xpath(XPATH):\n",
    "        url_list.append(a.get_attribute('href'))\n",
    "\n",
    "    for i in url_list:\n",
    "        print(i)\n",
    "    #driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://aumo.jp/articles/19756\n",
      "https://www.ozmall.co.jp/date/restaurant/cheesefondue/\n",
      "https://rtrp.jp/articles/53335/?page=3\n",
      "https://rtrp.jp/articles/8473/\n",
      "https://www.navitime.co.jp/poi?spt=01125.J000989271\n",
      "https://www.jalan.net/news/article/367303/\n",
      "https://r.gnavi.co.jp/landmark/0015543/cafe/rs/\n",
      "https://r.gnavi.co.jp/landmark/0015543/cheesefondu/rs/\n",
      "https://restaurant.ikyu.com/rsSpcl/sp/xmas/area/tokyo.htm\n",
      "https://tabelog.com/matome/tokyo/list/kwd30/CC0101/\n"
     ]
    }
   ],
   "source": [
    "word_list = [\"中野駅\", \"デートスポット\", \"チーズケーキ\"]\n",
    "\n",
    "top_10_sites(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'url_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-4c76a862a0c7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murl_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#header = r.headers[\"Content-Encoding\"]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#header\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'url_list' is not defined"
     ]
    }
   ],
   "source": [
    "url = url_list[1]\n",
    "r = requests.get(url, stream=True)\n",
    "#header = r.headers[\"Content-Encoding\"]\n",
    "#header\n",
    "\n",
    "text = r.text\n",
    "\n",
    "'''text_list = []\n",
    "for index, text in enumerate(url_list):\n",
    "    r = requests.get(url_list[index])\n",
    "    text_list.append(r.text)'''\n",
    "\n",
    "for line in text.split(\"/n\"):\n",
    "    #if \"<title>\" in line or \"<h1>\" in line:\n",
    "    if \"<p>\" in line:\n",
    "        print(line.strip())\n",
    "\n",
    "text\n",
    "#header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kanji = re.findall(\"[ぁ-龥　、　。]\", text)\n",
    "kanji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_colwidth\", 400)\n",
    "\n",
    "\n",
    "# ひらがなの抽出\n",
    "hiragana = re.findall(\"[ぁ-ん]\", text)\n",
    "# カタカナの抽出\n",
    "katakana = re.findall(\"[ァ-ン]\", text)\n",
    "# 漢字の抽出\n",
    "kanji = re.findall(\"[ぁ-龥　、　。]\", text)\n",
    "hiragana\n",
    "katakana\n",
    "joined_text = \"\".join(kanji)\n",
    "\n",
    "new_text = joined_text.split(\"。\")\n",
    "for index, line in enumerate(new_text):\n",
    "    new_text[index] += \"。\" \n",
    "\n",
    "\n",
    "for i in new_text:\n",
    "    if len(i) > 400:\n",
    "        new_text.remove(i)\n",
    "\n",
    "for i in new_text:\n",
    "    while new_text.count(i) > 1:\n",
    "        new_text.remove(i)\n",
    "        \n",
    "len(new_text)\n",
    "new_text\n",
    "df = pd.DataFrame(new_text)\n",
    "\n",
    "dfStyler = df.style.set_properties(**{'text-align': 'left'})\n",
    "dfStyler.set_table_styles([dict(selector='th', props=[('text-align', 'left')])])\n",
    "\n",
    "df\n",
    "\n",
    "df.to_csv(\"test.csv\", encoding='utf_8_sig')\n",
    "\n",
    "df\n",
    "\n",
    "new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_text\n",
    "len(new_text)\n",
    "type(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import MeCab\n",
    "import pandas as pd\n",
    "\n",
    "tagger = MeCab.Tagger(\"-Ochasen\")\n",
    "\n",
    "result_list = []\n",
    "for index, line in enumerate(new_text):\n",
    "    text = new_text[index]\n",
    "    #print(index)\n",
    "    #print(line)\n",
    "    #print(tagger.parse(line))\n",
    "    result_list.append(tagger.parse(line))\n",
    "    \n",
    "'''text = \"ブルーボトルコーヒーがオープンするなど今話題の清澄白河。\"\n",
    "result = tagger.parse(text)\n",
    "\n",
    "type(result)\n",
    "len(result)'''\n",
    "\n",
    "result_list = result.split(\"-\")\n",
    "df = pd.DataFrame(result_list)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "row = ['2', ' Marie', ' California']\n",
    "with open('people.csv', 'r') as readFile:\n",
    "    reader = csv.reader(readFile)\n",
    "    lines = list(reader)\n",
    "    lines[1] = row\n",
    "with open('people.csv', 'w') as writeFile:\n",
    "    writer = csv.writer(writeFile)\n",
    "    writer.writerows(lines)\n",
    "readFile.close()\n",
    "writeFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'header' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-660c35c52182>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavetxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"file_name.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'ブルーボトルコーヒーが'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'オープンするなど'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'今話題の清澄白河。'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\",\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfmt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'%s'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'header' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.savetxt(\"file_name.csv\", ['ブルーボトルコーヒーが','オープンするなど','今話題の清澄白河。'], delimiter=\",\", fmt='%s', header=header)\n",
    "\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "search_text1: Falseこちらに変換する文字を貼り付けて下さい。直接入力するか、ファイルからの読込み、ファイルドロップでも可能です。\n",
      "                \n",
      "search_text2: Falseこちらに変換する文字を貼り付けて下さい。直接入力するか、ファイルからの読込み、ファイルドロップでも可能です。\n",
      "                \n",
      "attrs_of_result: [['id', 'kekka'], ['name', 'ta2']]\n",
      "attrs_of_search: [['name', 'ta'], ['class', 'box0'], ['id', 'ta']]\n",
      "result_text4: \n"
     ]
    }
   ],
   "source": [
    "#OLD\n",
    "\n",
    "def pronounce(vocab_list):\n",
    "    import re\n",
    "    import requests\n",
    "    import time\n",
    "\n",
    "    from bs4 import BeautifulSoup\n",
    "    from random import random\n",
    "    from selenium import webdriver\n",
    "\n",
    "    driver = webdriver.Chrome()\n",
    "    just_a_sec = random() \n",
    "\n",
    "    search_site = \"https://www.webtoolss.com/hiragana.html\"\n",
    "\n",
    "    search_string = vocab_list\n",
    "    \n",
    "    driver.get(search_site)\n",
    "    time.sleep(just_a_sec * 1)\n",
    "    \n",
    "    #search_box = driver.find_element_by_name(\"ta\")\n",
    "    #OR\n",
    "    #search_box = driver.find_element_by_class_name(\"box0\")\n",
    "    #OR\n",
    "    search_box = driver.find_element_by_id(\"ta\")\n",
    "  \n",
    "    search_box.send_keys(search_string)\n",
    "    time.sleep(just_a_sec*1)\n",
    "\n",
    "    for i in driver.find_elements_by_class_name(\"btn\"):\n",
    "        if \"ふりがなに\" in i.text:\n",
    "            i.click()\n",
    "\n",
    "    time.sleep(just_a_sec*1)\n",
    "\n",
    "    result_box = driver.find_element_by_name(\"ta2\")\n",
    "    #OR\n",
    "    #result_box = driver.find_element_by_xpath('/html/body/div/article[1]/div[2]/textarea')\n",
    "    #OR\n",
    "    #result_box = driver.find_element_by_xpath('//*[@id=\"kekka\"]')\n",
    "    #OR\n",
    "    #result_box = driver.find_element_by_id(\"kekka\")\n",
    "    \n",
    "#    counter = 0\n",
    "#    result_list = []\n",
    "#   for i in driver.find_elements_by_id(\"kekka\"):\n",
    "#        print(counter)\n",
    "#        counter += 1\n",
    "#        result_list += result_list.append(i.text)\n",
    "\n",
    "\n",
    "    #result_box.send_keys(search_string)\n",
    "    \n",
    "    #result_box.clear()\n",
    "    #print(result_box)\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    search_text1 = search_box.get_attribute(\"textContent\")\n",
    "    search_text2 = search_box.text\n",
    "    ###search_text2 = search_box.get_attribute(\"innerText\")\n",
    "    ###search_text3 = search_box.get_attribute(\"outerText\")\n",
    "    print(\"search_text1: \" + str(search_text1 == True) + search_text1)\n",
    "    print(\"search_text2: \" + str(search_text2 == True) + search_text2)\n",
    "\n",
    "    \n",
    "    attrs_of_result = []\n",
    "    for attr in result_box.get_property('attributes'):\n",
    "        attrs_of_result.append([attr['name'], attr['value']])\n",
    "    print(\"attrs_of_result: \" + str(attrs_of_result))\n",
    "    \n",
    "    attrs_of_search = []\n",
    "    for attr in search_box.get_property('attributes'):\n",
    "        attrs_of_search.append([attr['name'], attr['value']])\n",
    "    print(\"attrs_of_search: \" + str(attrs_of_search))\n",
    "    \n",
    "    ###result_text1 = result_box.get_attribute(\"textContent\")\n",
    "    ###result_text2 = result_box.get_attribute(\"innerText\")\n",
    "    ###result_text3 = result_box.get_attribute(\"outerText\")\n",
    "    result_text4 = result_box.text\n",
    "    ###result_text5 = result_box.innerText\n",
    "    ###print(\"result_text1: \" + str(result_text1 == True) + result_text1)\n",
    "    ###print(\"result_text2: \" + str(result_text2 == True) + result_text2)\n",
    "    ###print(\"result_text3: \" + str(result_text3 == True) + result_text3)\n",
    "    print(\"result_text4: \" + result_text4)\n",
    "    ###print(\"result_text5: \" + result_text5)\n",
    "\n",
    "    \n",
    "    #return search_box.get_attribute(\"innerHTML\")\n",
    "    #OR\n",
    "    #return search_box.get_attribute(\"textContent\")\n",
    "    #return search_box.get_attribute(\"outerHTML\")\n",
    "\n",
    "sentence = \"私は台湾出身のエンジニアです。\"\n",
    "pronounce(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result_box.text: \n",
      "result_box.tag_name: textarea\n",
      "result_box.size: {'height': 200, 'width': 440}\n"
     ]
    }
   ],
   "source": [
    "def pronounce(vocab_list):\n",
    "    import re\n",
    "    import requests\n",
    "    import time\n",
    "\n",
    "    from bs4 import BeautifulSoup\n",
    "    from random import random\n",
    "    from selenium import webdriver\n",
    "\n",
    "    driver = webdriver.Chrome()\n",
    "    just_a_sec = random() \n",
    "\n",
    "    search_site = \"https://www.webtoolss.com/hiragana.html\"\n",
    "\n",
    "    search_string = vocab_list\n",
    "    \n",
    "    driver.get(search_site)\n",
    "    time.sleep(just_a_sec * 1)\n",
    "    \n",
    "    #search_box = driver.find_element_by_name(\"ta\")\n",
    "    #OR\n",
    "    #search_box = driver.find_element_by_class_name(\"box0\")\n",
    "    #OR\n",
    "    search_box = driver.find_element_by_id(\"ta\")\n",
    "  \n",
    "    search_box.send_keys(search_string)\n",
    "    time.sleep(just_a_sec*1)\n",
    "\n",
    "    for i in driver.find_elements_by_class_name(\"btn\"):\n",
    "        if \"ふりがなに\" in i.text:\n",
    "            i.click()\n",
    "\n",
    "    time.sleep(just_a_sec*1)\n",
    "\n",
    "    result_box = driver.find_element_by_name(\"ta2\")\n",
    "    #OR\n",
    "    #result_box = driver.find_element_by_xpath('/html/body/div/article[1]/div[2]/textarea')\n",
    "    #OR\n",
    "    #result_box = driver.find_element_by_xpath('//*[@id=\"kekka\"]')\n",
    "    #OR\n",
    "    #result_box = driver.find_element_by_id(\"kekka\")\n",
    "    \n",
    "#     search_text1 = search_box.get_attribute(\"textContent\")\n",
    "#     print(\"search_text1: \" + search_text1)\n",
    "#     search_text2 = search_box.get_attribute(\"outerHTML\")\n",
    "#     print(\"search_text2: \" + search_text2)\n",
    "#     search_text3 = search_box.get_attribute(\"innerHTML\")\n",
    "#     print(\"search_text3: \" + search_text3)\n",
    "#     search_text4 = search_box.text\n",
    "#     print(\"search_text4: \" + search_text4)\n",
    "    \n",
    "#     result_text1 = result_box.get_attribute(\"outerHTML\")\n",
    "#     print(\"result_text1: \" + result_text1)\n",
    "#     result_text2 = result_box.get_attribute(\"innerHTML\")\n",
    "#     print(\"result_text2: \" + result_text2)\n",
    "#     result_text3 = result_box.get_attribute(\"textContent\")\n",
    "#     print(\"result_text3: \" + result_text3)\n",
    "    result_text4 = result_box.text\n",
    "    print(\"result_box.text: \" + result_text4)\n",
    "    result_text5 = result_box.tag_name\n",
    "    print(\"result_box.tag_name: \" + result_text5)\n",
    "    result_text6 = result_box.size\n",
    "    print(\"result_box.size: \" + str(result_text6))\n",
    "\n",
    "#    attrs_of_result = []\n",
    "#    for attr in result_box.get_property('attributes'):\n",
    "#        attrs_of_result.append([attr['name'], attr['value']])\n",
    "#    print(\"attrs_of_result: \" + str(attrs_of_result))\n",
    "    \n",
    "sentence = \"検証用のコードを確認中です。\"\n",
    "pronounce(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result_furigana.text: きゅう しゅう\n",
      "result_stress.text:きうしう\n"
     ]
    }
   ],
   "source": [
    "def get_stress(vocab_list):\n",
    "    import re\n",
    "    import requests\n",
    "    import time\n",
    "\n",
    "    from bs4 import BeautifulSoup\n",
    "    from random import random\n",
    "    from selenium import webdriver\n",
    "\n",
    "    driver = webdriver.Chrome()\n",
    "    just_a_sec = random() \n",
    "    search_string = vocab_list\n",
    "    search_site = \"https://www.weblio.jp/content/\"\n",
    "    search_site = search_site + vocab_list\n",
    "    \n",
    "    \n",
    "    driver.get(search_site)\n",
    "    time.sleep(just_a_sec * 1)\n",
    "    \n",
    "    search_box = driver.find_element_by_class_name(\"formBox\")\n",
    "\n",
    "    result_furigana = driver.find_element_by_xpath('//*[@id=\"cont\"]/div[3]/div/div[1]/h2/b')\n",
    "    result_stress = driver.find_element_by_xpath('//*[@id=\"cont\"]/div[3]/div/div[1]/h2/span')\n",
    "                                                                                  \n",
    "    result_text4 = result_furigana.text\n",
    "    print(\"result_furigana.text: \" + result_text4)\n",
    "    result_text5 = result_stress.text\n",
    "    print(\"result_stress.text:\" + result_text5)\n",
    "\n",
    "#    attrs_of_result = []\n",
    "#    for attr in result_box.get_property('attributes'):\n",
    "#        attrs_of_result.append([attr['name'], attr['value']])\n",
    "#    print(\"attrs_of_result: \" + str(attrs_of_result))\n",
    "    \n",
    "sentence = \"九州\"\n",
    "get_stress(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'あおしま   しゅんさく'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pykakasi import kakasi\n",
    " \n",
    "text = \"青島   俊作\"\n",
    "\n",
    "\n",
    "kakasi = kakasi()\n",
    "# kakasi.setMode('J', 'K')\n",
    "# kakasi.setMode('J', 'a')\n",
    "kakasi.setMode('J', 'H')\n",
    "conv = kakasi.getConverter()\n",
    " \n",
    "conv.do(text) # アオシマシュンサク"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#K\tカタカナ\n",
    "#H\tひらがな\n",
    "#J\t漢字\n",
    "#a\tローマ字\n",
    "#E\tわからん"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<ipython-input-30-02844aec8fd1>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-30-02844aec8fd1>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    with open(\"インフルエンザ\") as file_flu:\u001b[0m\n\u001b[0m                                      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "with open(\"インフルエンザ\") as file_flu:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>単語</th>\n",
       "      <th>ふりがな</th>\n",
       "      <th>品詞</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>ワクチン</td>\n",
       "      <td>ワクチン</td>\n",
       "      <td>名詞</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>接種</td>\n",
       "      <td>セッシュ</td>\n",
       "      <td>名詞</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>インフルエンザ</td>\n",
       "      <td>インフルエンザ</td>\n",
       "      <td>名詞</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>予防</td>\n",
       "      <td>ヨボウ</td>\n",
       "      <td>名詞</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>接種</td>\n",
       "      <td>セッシュ</td>\n",
       "      <td>名詞</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>実施</td>\n",
       "      <td>ジッシ</td>\n",
       "      <td>名詞</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>する</td>\n",
       "      <td>スル</td>\n",
       "      <td>動詞</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>受け</td>\n",
       "      <td>ウケ</td>\n",
       "      <td>動詞</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>られる</td>\n",
       "      <td>ラレル</td>\n",
       "      <td>動詞</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>方</td>\n",
       "      <td>ホウ</td>\n",
       "      <td>名詞</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>健康</td>\n",
       "      <td>ケンコウ</td>\n",
       "      <td>名詞</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>状態</td>\n",
       "      <td>ジョウタイ</td>\n",
       "      <td>名詞</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>よく</td>\n",
       "      <td>ヨク</td>\n",
       "      <td>副詞</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>把握</td>\n",
       "      <td>ハアク</td>\n",
       "      <td>名詞</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>する</td>\n",
       "      <td>スル</td>\n",
       "      <td>動詞</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>必要</td>\n",
       "      <td>ヒツヨウ</td>\n",
       "      <td>名詞</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>あり</td>\n",
       "      <td>アリ</td>\n",
       "      <td>動詞</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>ます</td>\n",
       "      <td>マス</td>\n",
       "      <td>助動</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>その</td>\n",
       "      <td>ソノ</td>\n",
       "      <td>連体</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>ため</td>\n",
       "      <td>タメ</td>\n",
       "      <td>名詞</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>予診</td>\n",
       "      <td>ヨシン</td>\n",
       "      <td>名詞</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>票</td>\n",
       "      <td>ヒョウ</td>\n",
       "      <td>名詞</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>出来る</td>\n",
       "      <td>デキル</td>\n",
       "      <td>動詞</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>詳しく</td>\n",
       "      <td>クワシク</td>\n",
       "      <td>形容詞</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>ご</td>\n",
       "      <td>ゴ</td>\n",
       "      <td>接頭詞</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>記入</td>\n",
       "      <td>キニュウ</td>\n",
       "      <td>名詞</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>ください</td>\n",
       "      <td>クダサイ</td>\n",
       "      <td>動詞</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>&lt;</td>\n",
       "      <td>&lt;</td>\n",
       "      <td>名詞</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>ワクチン</td>\n",
       "      <td>ワクチン</td>\n",
       "      <td>名詞</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>効果</td>\n",
       "      <td>コウカ</td>\n",
       "      <td>名詞</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>副</td>\n",
       "      <td>フク</td>\n",
       "      <td>接頭詞</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>反応</td>\n",
       "      <td>ハンノウ</td>\n",
       "      <td>名詞</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>&gt;</td>\n",
       "      <td>&gt;</td>\n",
       "      <td>名詞</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>EOS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         単語     ふりがな   品詞\n",
       "0      ワクチン     ワクチン   名詞\n",
       "2        接種     セッシュ   名詞\n",
       "4   インフルエンザ  インフルエンザ   名詞\n",
       "6        予防      ヨボウ   名詞\n",
       "7        接種     セッシュ   名詞\n",
       "9        実施      ジッシ   名詞\n",
       "10       する       スル   動詞\n",
       "13       受け       ウケ   動詞\n",
       "14      られる      ラレル   動詞\n",
       "15        方       ホウ   名詞\n",
       "17       健康     ケンコウ   名詞\n",
       "18       状態    ジョウタイ   名詞\n",
       "20       よく       ヨク   副詞\n",
       "21       把握      ハアク   名詞\n",
       "22       する       スル   動詞\n",
       "23       必要     ヒツヨウ   名詞\n",
       "25       あり       アリ   動詞\n",
       "26       ます       マス   助動\n",
       "28       その       ソノ   連体\n",
       "29       ため       タメ   名詞\n",
       "31       予診      ヨシン   名詞\n",
       "32        票      ヒョウ   名詞\n",
       "34      出来る      デキル   動詞\n",
       "36      詳しく     クワシク  形容詞\n",
       "37        ご        ゴ  接頭詞\n",
       "38       記入     キニュウ   名詞\n",
       "39     ください     クダサイ   動詞\n",
       "41        <        <   名詞\n",
       "42     ワクチン     ワクチン   名詞\n",
       "44       効果      コウカ   名詞\n",
       "46        副       フク  接頭詞\n",
       "47       反応     ハンノウ   名詞\n",
       "48        >        >   名詞\n",
       "49      EOS      NaN  NaN"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def process_with_mecab(input_file_name, output_file_name):\n",
    "    import MeCab\n",
    "    \n",
    "    tagger = MeCab.Tagger(\"-Ochasen\")\n",
    "    \n",
    "    with open(input_file_name, encoding=\"utf-8\") as input_file:\n",
    "        with open(output_file_name, mode=\"w\", encoding=\"utf-8\") as output_file:\n",
    "            output_file.write(tagger.parse(input_file.read()))\n",
    "\n",
    "input_data_name = \"flu.txt\"\n",
    "output_data_name = input_data_name + \".mecab\"\n",
    "\n",
    "process_with_mecab(input_data_name, output_data_name)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "with open(\"flu.txt.mecab\") as file:\n",
    "    flu_nparray=np.array([file.read()])\n",
    "    flu_df = pd.DataFrame(flu_nparray)\n",
    "    \n",
    "import pandas as pd\n",
    "data = pd.read_table('flu.txt.mecab')\n",
    "data.columns = np.arange(0,6)\n",
    "data_nparray = np.array(data)\n",
    "#print(data_nparray.shape)\n",
    "data[0]\n",
    "\n",
    "sorted_data = data[[0,1,3]]\n",
    "sorted_data.columns = [\"単語\",\"ふりがな\",\"品詞\"]\n",
    "\n",
    "new_sorted_data = sorted_data.copy()\n",
    "\n",
    "POS_series = sorted_data[\"品詞\"]\n",
    "#new_POS_series=POS_series[:] #reference!\n",
    "new_POS_series=POS_series.copy()\n",
    "\n",
    "for index, POS in enumerate(new_POS_series):\n",
    "    word = new_POS_series[index]\n",
    "    if(type(word) == str):\n",
    "        pos_of_hyphen = word.find(\"-\")\n",
    "        new_POS_series[index] = word[:pos_of_hyphen]\n",
    "\n",
    "POS_series\n",
    "new_POS_series\n",
    "\n",
    "new_sorted_data[\"品詞\"] = new_POS_series\n",
    "new_sorted_data\n",
    "\n",
    "new_sorted_data.loc[1, \"品詞\"]\n",
    "row_count = new_sorted_data.shape[0]\n",
    "\n",
    "functional_word_index = []\n",
    "for i in np.arange(row_count):\n",
    "    if new_sorted_data.loc[i, \"品詞\"] in [\"助詞\", \"記号\"]:\n",
    "        functional_word_index.append(i)\n",
    "        #new_sorted_data.drop([functional_word_index])\n",
    "\n",
    "#print(functional_word_index)\n",
    "new_sorted_data.drop(functional_word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"None of [Int64Index([0, 1, 3], dtype='int64')] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-224-946e63ba9297>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0msorted_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;31m# sorted_data.columns = [\"単語\",\"ふりがな\",\"品詞\"]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# new_sorted_data = sorted_data.copy()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2984\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2985\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2986\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_to_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2987\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2988\u001b[0m         \u001b[0;31m# take() does not accept boolean indexers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_convert_to_indexer\u001b[0;34m(self, obj, axis, is_setter, raise_missing)\u001b[0m\n\u001b[1;32m   1283\u001b[0m                 \u001b[0;31m# When setting, missing keys are not allowed, even with .loc:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1284\u001b[0m                 \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"raise_missing\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_setter\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1285\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_listlike_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1286\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1287\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_get_listlike_indexer\u001b[0;34m(self, key, axis, raise_missing)\u001b[0m\n\u001b[1;32m   1090\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1091\u001b[0m         self._validate_read_indexer(\n\u001b[0;32m-> 1092\u001b[0;31m             \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_axis_number\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mraise_missing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1093\u001b[0m         )\n\u001b[1;32m   1094\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_validate_read_indexer\u001b[0;34m(self, key, indexer, axis, raise_missing)\u001b[0m\n\u001b[1;32m   1175\u001b[0m                 raise KeyError(\n\u001b[1;32m   1176\u001b[0m                     \"None of [{key}] are in the [{axis}]\".format(\n\u001b[0;32m-> 1177\u001b[0;31m                         \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_axis_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1178\u001b[0m                     )\n\u001b[1;32m   1179\u001b[0m                 )\n",
      "\u001b[0;31mKeyError\u001b[0m: \"None of [Int64Index([0, 1, 3], dtype='int64')] are in the [columns]\""
     ]
    }
   ],
   "source": [
    "import MeCab\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def mecab_parse(file_in, file_out):\n",
    "    \n",
    "    import MeCab\n",
    "    tagger = MeCab.Tagger(\"-Ochasen\")\n",
    "    \n",
    "    with open(file_in, encoding=\"utf-8\") as input_file:\n",
    "        with open(file_out, mode=\"w\", encoding=\"utf-8\") as output_file:\n",
    "            output_file.write(tagger.parse(input_file.read()))\n",
    "\n",
    "file_in = \"flu.txt\"\n",
    "file_out = file_in + \".mecab\"\n",
    "mecab_parse(file_in, file_out)\n",
    "    \n",
    "data = pd.read_table('flu.txt.mecab')\n",
    "\n",
    "\n",
    "sorted_data = data[[0,1,3]].copy()\n",
    "# sorted_data.columns = [\"単語\",\"ふりがな\",\"品詞\"]\n",
    "# new_sorted_data = sorted_data.copy()\n",
    "\n",
    "# POS_series = sorted_data[\"品詞\"]\n",
    "\n",
    "\n",
    "# new_POS_series=POS_series.copy() ##new_POS_series=POS_series[:] #reference!\n",
    "\n",
    "# for index, POS in enumerate(new_POS_series):\n",
    "#     word = new_POS_series[index]\n",
    "#     if(type(word) == str):\n",
    "#         pos_of_hyphen = word.find(\"-\")\n",
    "#         new_POS_series[index] = word[:pos_of_hyphen]\n",
    "\n",
    "# new_sorted_data[\"品詞\"] = new_POS_series\n",
    "\n",
    "# new_sorted_data.loc[1, \"品詞\"]\n",
    "# row_count = new_sorted_data.shape[0]\n",
    "\n",
    "# functional_word_index = []\n",
    "# for i in np.arange(row_count):\n",
    "#     if new_sorted_data.loc[i, \"品詞\"] in [\"助詞\", \"記号\"]:\n",
    "#         functional_word_index.append(i)\n",
    "        \n",
    "# new_sorted_data.drop(functional_word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
