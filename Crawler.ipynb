{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://qiita.com/memakura/items/20a02161fa7e18d8a693\n",
    "#https://qiita.com/y__ueda/items/7b6f2a95ea45667e1029\n",
    "\n",
    "def top_10_sites(vocab_list):\n",
    "    import re\n",
    "    import requests\n",
    "    import time\n",
    "\n",
    "    from bs4 import BeautifulSoup\n",
    "    from random import random\n",
    "    from selenium import webdriver\n",
    "\n",
    "    driver = webdriver.Chrome()\n",
    "    just_a_sec = random() \n",
    "\n",
    "    #vocab_list = [\"浅草\", \"デートスポット\", \"雑貨\"]\n",
    "    search_string = \"\"\n",
    "    for i in vocab_list:\n",
    "        search_string = search_string + i + '\"　\"'\n",
    "\n",
    "    driver.get('https://www.google.com/')\n",
    "    time.sleep(just_a_sec * 1)\n",
    "    search_box = driver.find_element_by_name(\"q\")\n",
    "    search_box.send_keys('\"' + search_string + '\"')\n",
    "    time.sleep(just_a_sec*1)\n",
    "    search_box.submit()\n",
    "    time.sleep(just_a_sec*1)\n",
    "\n",
    "    #for h1 in driver.find_elements_by_tag_name(\"h1\"):\n",
    "    #    print(h1.text)\n",
    "\n",
    "    #i = 0\n",
    "    url_list = []\n",
    "    XPATH = '//*[@id=\"rso\"]/div/div/div[\" + str(i + 1) + \"]/div/div/div[1]/a'\n",
    "    for a in driver.find_elements_by_xpath(XPATH):\n",
    "        url_list.append(a.get_attribute('href'))\n",
    "\n",
    "    for i in url_list:\n",
    "        print(i)\n",
    "    #driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://aumo.jp/articles/19756\n",
      "https://www.ozmall.co.jp/date/restaurant/cheesefondue/\n",
      "https://rtrp.jp/articles/53335/?page=3\n",
      "https://rtrp.jp/articles/8473/\n",
      "https://www.navitime.co.jp/poi?spt=01125.J000989271\n",
      "https://www.jalan.net/news/article/367303/\n",
      "https://r.gnavi.co.jp/landmark/0015543/cafe/rs/\n",
      "https://r.gnavi.co.jp/landmark/0015543/cheesefondu/rs/\n",
      "https://restaurant.ikyu.com/rsSpcl/sp/xmas/area/tokyo.htm\n",
      "https://tabelog.com/matome/tokyo/list/kwd30/CC0101/\n"
     ]
    }
   ],
   "source": [
    "word_list = [\"中野駅\", \"デートスポット\", \"チーズケーキ\"]\n",
    "\n",
    "top_10_sites(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'url_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-4c76a862a0c7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murl_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#header = r.headers[\"Content-Encoding\"]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#header\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'url_list' is not defined"
     ]
    }
   ],
   "source": [
    "url = url_list[1]\n",
    "r = requests.get(url, stream=True)\n",
    "#header = r.headers[\"Content-Encoding\"]\n",
    "#header\n",
    "\n",
    "text = r.text\n",
    "\n",
    "'''text_list = []\n",
    "for index, text in enumerate(url_list):\n",
    "    r = requests.get(url_list[index])\n",
    "    text_list.append(r.text)'''\n",
    "\n",
    "for line in text.split(\"/n\"):\n",
    "    #if \"<title>\" in line or \"<h1>\" in line:\n",
    "    if \"<p>\" in line:\n",
    "        print(line.strip())\n",
    "\n",
    "text\n",
    "#header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kanji = re.findall(\"[ぁ-龥　、　。]\", text)\n",
    "kanji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_colwidth\", 400)\n",
    "\n",
    "\n",
    "# ひらがなの抽出\n",
    "hiragana = re.findall(\"[ぁ-ん]\", text)\n",
    "# カタカナの抽出\n",
    "katakana = re.findall(\"[ァ-ン]\", text)\n",
    "# 漢字の抽出\n",
    "kanji = re.findall(\"[ぁ-龥　、　。]\", text)\n",
    "hiragana\n",
    "katakana\n",
    "joined_text = \"\".join(kanji)\n",
    "\n",
    "new_text = joined_text.split(\"。\")\n",
    "for index, line in enumerate(new_text):\n",
    "    new_text[index] += \"。\" \n",
    "\n",
    "\n",
    "for i in new_text:\n",
    "    if len(i) > 400:\n",
    "        new_text.remove(i)\n",
    "\n",
    "for i in new_text:\n",
    "    while new_text.count(i) > 1:\n",
    "        new_text.remove(i)\n",
    "        \n",
    "len(new_text)\n",
    "new_text\n",
    "df = pd.DataFrame(new_text)\n",
    "\n",
    "dfStyler = df.style.set_properties(**{'text-align': 'left'})\n",
    "dfStyler.set_table_styles([dict(selector='th', props=[('text-align', 'left')])])\n",
    "\n",
    "df\n",
    "\n",
    "df.to_csv(\"test.csv\", encoding='utf_8_sig')\n",
    "\n",
    "df\n",
    "\n",
    "new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_text\n",
    "len(new_text)\n",
    "type(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import MeCab\n",
    "import pandas as pd\n",
    "\n",
    "tagger = MeCab.Tagger(\"-Ochasen\")\n",
    "\n",
    "result_list = []\n",
    "for index, line in enumerate(new_text):\n",
    "    text = new_text[index]\n",
    "    #print(index)\n",
    "    #print(line)\n",
    "    #print(tagger.parse(line))\n",
    "    result_list.append(tagger.parse(line))\n",
    "    \n",
    "'''text = \"ブルーボトルコーヒーがオープンするなど今話題の清澄白河。\"\n",
    "result = tagger.parse(text)\n",
    "\n",
    "type(result)\n",
    "len(result)'''\n",
    "\n",
    "result_list = result.split(\"-\")\n",
    "df = pd.DataFrame(result_list)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "row = ['2', ' Marie', ' California']\n",
    "with open('people.csv', 'r') as readFile:\n",
    "    reader = csv.reader(readFile)\n",
    "    lines = list(reader)\n",
    "    lines[1] = row\n",
    "with open('people.csv', 'w') as writeFile:\n",
    "    writer = csv.writer(writeFile)\n",
    "    writer.writerows(lines)\n",
    "readFile.close()\n",
    "writeFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'header' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-660c35c52182>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavetxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"file_name.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'ブルーボトルコーヒーが'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'オープンするなど'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'今話題の清澄白河。'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\",\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfmt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'%s'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'header' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.savetxt(\"file_name.csv\", ['ブルーボトルコーヒーが','オープンするなど','今話題の清澄白河。'], delimiter=\",\", fmt='%s', header=header)\n",
    "\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result_box.text: \n",
      "result_box.tag_name: textarea\n",
      "result_box.size: {'height': 200, 'width': 440}\n"
     ]
    }
   ],
   "source": [
    "#FAILED\n",
    "\n",
    "def pronounce(vocab_list):\n",
    "    import re\n",
    "    import requests\n",
    "    import time\n",
    "\n",
    "    from bs4 import BeautifulSoup\n",
    "    from random import random\n",
    "    from selenium import webdriver\n",
    "\n",
    "    driver = webdriver.Chrome()\n",
    "    just_a_sec = random() \n",
    "\n",
    "    search_site = \"https://www.webtoolss.com/hiragana.html\"\n",
    "\n",
    "    search_string = vocab_list\n",
    "    \n",
    "    driver.get(search_site)\n",
    "    time.sleep(just_a_sec * 1)\n",
    "    \n",
    "    #search_box = driver.find_element_by_name(\"ta\")\n",
    "    #OR\n",
    "    #search_box = driver.find_element_by_class_name(\"box0\")\n",
    "    #OR\n",
    "    search_box = driver.find_element_by_id(\"ta\")\n",
    "  \n",
    "    search_box.send_keys(search_string)\n",
    "    time.sleep(just_a_sec*1)\n",
    "\n",
    "    for i in driver.find_elements_by_class_name(\"btn\"):\n",
    "        if \"ふりがなに\" in i.text:\n",
    "            i.click()\n",
    "\n",
    "    time.sleep(just_a_sec*1)\n",
    "\n",
    "    result_box = driver.find_element_by_name(\"ta2\")\n",
    "    #OR\n",
    "    #result_box = driver.find_element_by_xpath('/html/body/div/article[1]/div[2]/textarea')\n",
    "    #OR\n",
    "    #result_box = driver.find_element_by_xpath('//*[@id=\"kekka\"]')\n",
    "    #OR\n",
    "    #result_box = driver.find_element_by_id(\"kekka\")\n",
    "\n",
    "    result_text4 = result_box.text\n",
    "    print(\"result_box.text: \" + result_text4)\n",
    "    result_text5 = result_box.tag_name\n",
    "    print(\"result_box.tag_name: \" + result_text5)\n",
    "    result_text6 = result_box.size\n",
    "    print(\"result_box.size: \" + str(result_text6))\n",
    "\n",
    "#    attrs_of_result = []\n",
    "#    for attr in result_box.get_property('attributes'):\n",
    "#        attrs_of_result.append([attr['name'], attr['value']])\n",
    "#    print(\"attrs_of_result: \" + str(attrs_of_result))\n",
    "    \n",
    "sentence = \"検証用のコードを確認中です。\"\n",
    "pronounce(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Johnson',\n",
       " 'Stevens',\n",
       " 'じんましん',\n",
       " 'アナフィラキシー',\n",
       " 'アレルギー',\n",
       " 'アレルギー性',\n",
       " 'インフルエンザ',\n",
       " 'ニューロパチー',\n",
       " 'ネフローゼ',\n",
       " '一',\n",
       " '一方',\n",
       " '一般',\n",
       " '一過',\n",
       " '万',\n",
       " '下痢',\n",
       " '不',\n",
       " '両',\n",
       " '予診',\n",
       " '予防',\n",
       " '人',\n",
       " '今',\n",
       " '他',\n",
       " '以内',\n",
       " '低下',\n",
       " '体',\n",
       " '保健',\n",
       " '倦怠',\n",
       " '健康',\n",
       " '免疫',\n",
       " '入浴',\n",
       " '入院',\n",
       " '全身',\n",
       " '出',\n",
       " '出来',\n",
       " '分間',\n",
       " '判断',\n",
       " '前',\n",
       " '副',\n",
       " '効果',\n",
       " '動機',\n",
       " '化膿',\n",
       " '医師',\n",
       " '医療',\n",
       " '医薬品',\n",
       " '卵',\n",
       " '反応',\n",
       " '可能',\n",
       " '合併症',\n",
       " '吐',\n",
       " '呼吸',\n",
       " '嘱',\n",
       " '器',\n",
       " '困難',\n",
       " '場合',\n",
       " '多',\n",
       " '大量',\n",
       " '失神',\n",
       " '妊娠',\n",
       " '実施',\n",
       " '家族',\n",
       " '師',\n",
       " '庫',\n",
       " '当日',\n",
       " '形',\n",
       " '後',\n",
       " '徴',\n",
       " '心臓',\n",
       " '必要',\n",
       " '急',\n",
       " '急性',\n",
       " '性',\n",
       " '息',\n",
       " '悪寒',\n",
       " '意識',\n",
       " '感',\n",
       " '手足',\n",
       " '把握',\n",
       " '投与',\n",
       " '折',\n",
       " '指導',\n",
       " '指摘',\n",
       " '接種',\n",
       " '救済',\n",
       " '散在',\n",
       " '数',\n",
       " '斑',\n",
       " '方',\n",
       " '日',\n",
       " '旨',\n",
       " '期待',\n",
       " '末梢',\n",
       " '本人',\n",
       " '検査',\n",
       " '様',\n",
       " '様子',\n",
       " '機器',\n",
       " '機構',\n",
       " '機能',\n",
       " '機関',\n",
       " '次',\n",
       " '歩行',\n",
       " '歳',\n",
       " '死亡',\n",
       " '気',\n",
       " '気管支',\n",
       " '法',\n",
       " '法人',\n",
       " '注射',\n",
       " '注意',\n",
       " '浮腫',\n",
       " '消失',\n",
       " '清潔',\n",
       " '減少',\n",
       " '炎',\n",
       " '炎症',\n",
       " '熱',\n",
       " '熱性',\n",
       " '状態',\n",
       " '独立',\n",
       " '生活',\n",
       " '異常',\n",
       " '疾患',\n",
       " '疾病',\n",
       " '病',\n",
       " '病気',\n",
       " '症',\n",
       " '症候群',\n",
       " '症状',\n",
       " '痛',\n",
       " '発作',\n",
       " '発熱',\n",
       " '発疹',\n",
       " '発病',\n",
       " '発症',\n",
       " '発育',\n",
       " '白血球',\n",
       " '的',\n",
       " '皮膚',\n",
       " '相談',\n",
       " '眼',\n",
       " '破砕',\n",
       " '神経',\n",
       " '票',\n",
       " '程度',\n",
       " '等',\n",
       " '筋力',\n",
       " '筋肉',\n",
       " '節',\n",
       " '篤',\n",
       " '粘膜',\n",
       " '系',\n",
       " '紅',\n",
       " '紫',\n",
       " '紫斑',\n",
       " '細菌',\n",
       " '総合',\n",
       " '者',\n",
       " '聯',\n",
       " '肉芽',\n",
       " '肝',\n",
       " '肝臓',\n",
       " '肺炎',\n",
       " '脊髄',\n",
       " '脳',\n",
       " '脳炎',\n",
       " '脳症',\n",
       " '腎臓',\n",
       " '腫',\n",
       " '腹痛',\n",
       " '膜',\n",
       " '薬',\n",
       " '蜂巣',\n",
       " '血小板',\n",
       " '血液',\n",
       " '血管',\n",
       " '行政',\n",
       " '衰退',\n",
       " '被害',\n",
       " '見',\n",
       " '視神経',\n",
       " '観察',\n",
       " '記入',\n",
       " '診察',\n",
       " '質',\n",
       " '車',\n",
       " '軽',\n",
       " '軽減',\n",
       " '近親',\n",
       " '迷走',\n",
       " '通常',\n",
       " '連絡',\n",
       " '週間',\n",
       " '運動',\n",
       " '過去',\n",
       " '過敏',\n",
       " '適当',\n",
       " '部位',\n",
       " '重',\n",
       " '間',\n",
       " '関節',\n",
       " '際',\n",
       " '障害',\n",
       " '非常',\n",
       " '頭痛',\n",
       " '顔面',\n",
       " '食事',\n",
       " '食欲',\n",
       " '飲酒',\n",
       " '高熱',\n",
       " '鶏卵',\n",
       " '鶏肉',\n",
       " '麻',\n",
       " '黄']"
      ]
     },
     "execution_count": 420,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import MeCab\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import csv\n",
    "\n",
    "def mecab_parse(file_in, file_out = file_in + \".mecab\"):\n",
    "\n",
    "    tagger = MeCab.Tagger(\"-Ochasen\")\n",
    "    \n",
    "    with open(file_in, encoding=\"utf-8\") as input_file:\n",
    "        with open(file_out, mode=\"w\", encoding=\"utf-8\") as output_file:\n",
    "            #output_file.write(tagger.parse(input_file.read()))\n",
    "            parsed = tagger.parse(input_file.read())\n",
    "            output_file.write(parsed)\n",
    "\n",
    "    data = pd.read_table(file_out)\n",
    "    data.columns = np.arange(0,data.shape[1])\n",
    "    new_sorted_data = data[[0,1,3]].copy()\n",
    "\n",
    "    new_sorted_data.columns = [\"単語\",\"ふりがな\",\"品詞\"]\n",
    "    new_POS_series = new_sorted_data[\"品詞\"]\n",
    "\n",
    "    for index, POS in enumerate(new_POS_series):\n",
    "        word = new_POS_series[index]\n",
    "        if(type(word) == str):\n",
    "            pos_of_hyphen = word.find(\"-\")\n",
    "            new_POS_series[index] = word[:pos_of_hyphen]\n",
    "\n",
    "    new_sorted_data[\"品詞\"] = new_POS_series\n",
    "\n",
    "    new_sorted_data.loc[1, \"品詞\"]\n",
    "    row_count = new_sorted_data.shape[0]\n",
    "\n",
    "    functional_word_index = []\n",
    "    for i in np.arange(row_count):\n",
    "        pos_str =str(new_sorted_data.loc[i, \"品詞\"])\n",
    "        word_str = str(new_sorted_data.loc[i, \"単語\"])\n",
    "        \n",
    "        re_kanji = re.compile(r'^[\\u4E00-\\u9FD0]+$')\n",
    "        re_roman = re.compile(r'^[a-zA-Z]+$') #a-z:小文字、A-Z:大文字\n",
    "        re_katakana = re.compile(r'[\\u30A1-\\u30F4]+')\n",
    "        re_hiragana = re.compile(r'^[あ-ん]+$')\n",
    "\n",
    "        status_kanji = re_kanji.fullmatch(word_str)\n",
    "        \n",
    "        if pos_str in [\"助詞\", \"記号\"]:\n",
    "            functional_word_index.append(i)\n",
    "        elif status_kanji == None and len(word_str) < 5:\n",
    "            functional_word_index.append(i)\n",
    "    \n",
    "    word_table = new_sorted_data.drop(functional_word_index)\n",
    "    word_table = word_table.sort_values(by = \"単語\")\n",
    "    word_table = word_table.drop_duplicates()\n",
    "    word_table.index = np.arange(word_table.shape[0])\n",
    "    return word_table.iloc[:,:]\n",
    "\n",
    "result = mecab_parse(\"flu_detail.txt\")\n",
    "result\n",
    "\n",
    "#result.to_csv(\"result2.csv\" , encoding=\"utf-8\") #default: encoding=\"utf-8\"\n",
    "#result.to_csv(\"result2.csv\" , encoding=\"shift_jis\")\n",
    "\n",
    "vocab_list = result.iloc[:, 0]\n",
    "\n",
    "list_of_words = []\n",
    "count = 0\n",
    "for i in vocab_list:\n",
    "    list_of_words.append(i)\n",
    "\n",
    "list_of_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['[na]',\n",
       "  '[na]',\n",
       "  '［3］',\n",
       "  '［5］',\n",
       "  '［2］',\n",
       "  '[na]',\n",
       "  '［5］',\n",
       "  '［3］',\n",
       "  '［3］',\n",
       "  '［1］',\n",
       "  '－ぱう',\n",
       "  '［0］',\n",
       "  '－くわ',\n",
       "  '［1］',\n",
       "  '［0］',\n",
       "  '[na]',\n",
       "  '［1］',\n",
       "  '［0］',\n",
       "  '－ばう',\n",
       "  '［1］',\n",
       "  '[na]',\n",
       "  '［1］',\n",
       "  '［1］',\n",
       "  '［0］',\n",
       "  '［0］',\n",
       "  '［0］',\n",
       "  '［0］',\n",
       "  '－かう',\n",
       "  '［0］',\n",
       "  'にふ－',\n",
       "  'にふゐん',\n",
       "  '［0］',\n",
       "  '▽',\n",
       "  '［0］',\n",
       "  '［0］',\n",
       "  '［1］',\n",
       "  '［0］',\n",
       "  '［2］',\n",
       "  'かうくわ',\n",
       "  '［0］',\n",
       "  'くわ－',\n",
       "  '［1］',\n",
       "  '－れう',\n",
       "  '［0］',\n",
       "  'かひ',\n",
       "  '[na]',\n",
       "  '［0］',\n",
       "  '－しやう',\n",
       "  '[na]',\n",
       "  '－きふ',\n",
       "  '［1］',\n",
       "  'うつは',\n",
       "  '［1］',\n",
       "  '－あひ',\n",
       "  'さは',\n",
       "  '－りやう',\n",
       "  '［0］',\n",
       "  '［0］',\n",
       "  '［0］',\n",
       "  '［1］',\n",
       "  '[na]',\n",
       "  '［2］',\n",
       "  'たう－',\n",
       "  '［2］',\n",
       "  '［1］',\n",
       "  '［0］',\n",
       "  '－ざう',\n",
       "  '－えう',\n",
       "  'きふ',\n",
       "  'きふ－',\n",
       "  '［1］',\n",
       "  '［1］',\n",
       "  'を－',\n",
       "  '［1］',\n",
       "  '［1］',\n",
       "  '［1］',\n",
       "  '［0］',\n",
       "  '［1］',\n",
       "  'をり',\n",
       "  '－だう',\n",
       "  '［0］',\n",
       "  '［1］',\n",
       "  'きう－',\n",
       "  '［0］',\n",
       "  '[na]',\n",
       "  '▽',\n",
       "  'へ',\n",
       "  '[na]',\n",
       "  '［2］',\n",
       "  '［0］',\n",
       "  '－せう',\n",
       "  '［1］',\n",
       "  '［1］',\n",
       "  '［2］',\n",
       "  '［0］',\n",
       "  '［1］',\n",
       "  '［0］',\n",
       "  '［1］',\n",
       "  '［0］',\n",
       "  '[na]',\n",
       "  '－かう',\n",
       "  '[na]',\n",
       "  '－ばう',\n",
       "  '［0］',\n",
       "  '－くわん－',\n",
       "  '［2］',\n",
       "  'はふ－',\n",
       "  '［0］',\n",
       "  '［1］',\n",
       "  '［1］',\n",
       "  'せう－',\n",
       "  '［0］',\n",
       "  '－せう',\n",
       "  'ほのほ',\n",
       "  '－しやう',\n",
       "  '▽',\n",
       "  '［0］',\n",
       "  'じやう－',\n",
       "  '［0］',\n",
       "  '－くわつ',\n",
       "  '－じやう',\n",
       "  '－くわん',\n",
       "  '［0］',\n",
       "  'やまひ',\n",
       "  'びやう－',\n",
       "  'せう',\n",
       "  'しやう－',\n",
       "  'しやうじやう',\n",
       "  '［1］',\n",
       "  '［0］',\n",
       "  '［0］',\n",
       "  '［0］',\n",
       "  '－びやう',\n",
       "  '－しやう',\n",
       "  '［0］',\n",
       "  'はくけつきう',\n",
       "  '[na]',\n",
       "  '［1］',\n",
       "  'さう－',\n",
       "  '［1］',\n",
       "  '［0］',\n",
       "  '［1］',\n",
       "  'へう',\n",
       "  '［0］',\n",
       "  '[na]',\n",
       "  '［1］',\n",
       "  '［1］',\n",
       "  '[na]',\n",
       "  '［0］',\n",
       "  '［1］',\n",
       "  '［1］',\n",
       "  '－なゐ',\n",
       "  '［2］',\n",
       "  '［0］',\n",
       "  '［0］',\n",
       "  '－がふ',\n",
       "  '[na]',\n",
       "  '▼',\n",
       "  '［0］',\n",
       "  '［1］',\n",
       "  '－ざう',\n",
       "  '［0］',\n",
       "  '［2］',\n",
       "  'なづき',\n",
       "  'なう－',\n",
       "  'なうしやう',\n",
       "  '－ざう',\n",
       "  '［1］',\n",
       "  '［0］',\n",
       "  '［2］',\n",
       "  '［0］',\n",
       "  '［0］',\n",
       "  '－せうばん',\n",
       "  '［2］',\n",
       "  '－くわん',\n",
       "  'ぎやう－',\n",
       "  '［0］',\n",
       "  '［1］',\n",
       "  '［1］',\n",
       "  '［2］',\n",
       "  'くわん－',\n",
       "  '－にふ',\n",
       "  '［0］',\n",
       "  '［2］',\n",
       "  '［0］',\n",
       "  '[na]',\n",
       "  '［0］',\n",
       "  '［0］',\n",
       "  '［0］',\n",
       "  '－じやう',\n",
       "  '［0］',\n",
       "  'しう－',\n",
       "  '［0］',\n",
       "  'くわ－',\n",
       "  'くわ－',\n",
       "  '－たう',\n",
       "  '－ゐ',\n",
       "  'へ',\n",
       "  'あひ',\n",
       "  'くわん－',\n",
       "  'きは',\n",
       "  'しやう－',\n",
       "  '－じやう',\n",
       "  'づ－',\n",
       "  '［0］',\n",
       "  '［0］',\n",
       "  '［0］',\n",
       "  '［0］',\n",
       "  'かう－',\n",
       "  '［0］',\n",
       "  '［0］',\n",
       "  '［2］',\n",
       "  '［1］'],\n",
       " ['Johnson: [na]',\n",
       "  'Stevens: [na]',\n",
       "  'じんましん: じん ましん: ［3］',\n",
       "  'アナフィラキシー: アナフィラキシー: ［5］',\n",
       "  'アレルギー: アレルギー: ［2］',\n",
       "  'アレルギー性: [na]',\n",
       "  'インフルエンザ: インフルエンザ: ［5］',\n",
       "  'ニューロパチー: ニューロパチー: ［3］',\n",
       "  'ネフローゼ: ネフローゼ: ［3］',\n",
       "  '一: イー: ［1］',\n",
       "  '一方: いっ ぽう: －ぱう',\n",
       "  '一般: いっ ぱん: ［0］',\n",
       "  '一過: いっ か: －くわ',\n",
       "  '万: ばん: ［1］',\n",
       "  '下痢: げ り: ［0］',\n",
       "  '不: [na]',\n",
       "  '両: りゃん: ［1］',\n",
       "  '予診: よ しん: ［0］',\n",
       "  '予防: よ ぼう: －ばう',\n",
       "  '人: じん: ［1］',\n",
       "  '今: [na]',\n",
       "  '他: た: ［1］',\n",
       "  '以内: い ない: ［1］',\n",
       "  '低下: てい か: ［0］',\n",
       "  '体: からだ: ［0］',\n",
       "  '保健: ほ けん: ［0］',\n",
       "  '倦怠: けん たい: ［0］',\n",
       "  '健康: けん こう: －かう',\n",
       "  '免疫: めん えき: ［0］',\n",
       "  '入浴: にゅう よく: にふ－',\n",
       "  '入院: にゅう いん: にふゐん',\n",
       "  '全身: ぜん しん: ［0］',\n",
       "  '出: いだし: ▽',\n",
       "  '出来: しゅっ たい: ［0］',\n",
       "  '分間: ぶん けん: ［0］',\n",
       "  '判断: はん だん: ［1］',\n",
       "  '前: さき: ［0］',\n",
       "  '副: ふく: ［2］',\n",
       "  '効果: こう か: かうくわ',\n",
       "  '動機: どう き: ［0］',\n",
       "  '化膿: か のう: くわ－',\n",
       "  '医師: い し: ［1］',\n",
       "  '医療: い りょう: －れう',\n",
       "  '医薬品: い やくひん: ［0］',\n",
       "  '卵: かい: かひ',\n",
       "  '反応: [na]',\n",
       "  '可能: か のう: ［0］',\n",
       "  '合併症: がっ ぺいしょう: －しやう',\n",
       "  '吐: [na]',\n",
       "  '呼吸: こ きゅう: －きふ',\n",
       "  '嘱: しょく: ［1］',\n",
       "  '器: うつわ: うつは',\n",
       "  '困難: こん なん: ［1］',\n",
       "  '場合: ば あい: －あひ',\n",
       "  '多: さわ: さは',\n",
       "  '大量: たい りょう: －りやう',\n",
       "  '失神: しっ しん: ［0］',\n",
       "  '妊娠: にん しん: ［0］',\n",
       "  '実施: じっ し: ［0］',\n",
       "  '家族: か ぞく: ［1］',\n",
       "  '師: [na]',\n",
       "  '庫: くら: ［2］',\n",
       "  '当日: とう じつ: たう－',\n",
       "  '形: かた: ［2］',\n",
       "  '後: あと: ［1］',\n",
       "  '徴: しるし: ［0］',\n",
       "  '心臓: しん ぞう: －ざう',\n",
       "  '必要: ひつ よう: －えう',\n",
       "  '急: きゅう: きふ',\n",
       "  '急性: きゅう せい: きふ－',\n",
       "  '性: さが: ［1］',\n",
       "  '息: いき: ［1］',\n",
       "  '悪寒: お かん: を－',\n",
       "  '意識: い しき: ［1］',\n",
       "  '感: かん: ［1］',\n",
       "  '手足: しゅ そく: ［1］',\n",
       "  '把握: は あく: ［0］',\n",
       "  '投与: とう よ: ［1］',\n",
       "  '折: おり: をり',\n",
       "  '指導: し どう: －だう',\n",
       "  '指摘: し てき: ［0］',\n",
       "  '接種: せっ しゅ: ［1］',\n",
       "  '救済: きゅう さい: きう－',\n",
       "  '散在: さん ざい: ［0］',\n",
       "  '数: [na]',\n",
       "  '斑: はだら: ▽',\n",
       "  '方: え: へ',\n",
       "  '日: [na]',\n",
       "  '旨: むね: ［2］',\n",
       "  '期待: き たい: ［0］',\n",
       "  '末梢: まっ しょう: －せう',\n",
       "  '本人: ほん にん: ［1］',\n",
       "  '検査: けん さ: ［1］',\n",
       "  '様: さま: ［2］',\n",
       "  '様子: よう す: ［0］',\n",
       "  '機器: き き: ［1］',\n",
       "  '機構: き こう: ［0］',\n",
       "  '機能: き のう: ［1］',\n",
       "  '機関: から くり: ［0］',\n",
       "  '次: [na]',\n",
       "  '歩行: ほ こう: －かう',\n",
       "  '歳: [na]',\n",
       "  '死亡: し ぼう: －ばう',\n",
       "  '気: き: ［0］',\n",
       "  '気管支: き かんし: －くわん－',\n",
       "  '法: のり: ［2］',\n",
       "  '法人: ほう じん: はふ－',\n",
       "  '注射: ちゅう しゃ: ［0］',\n",
       "  '注意: ちゅう い: ［1］',\n",
       "  '浮腫: ふ しゅ: ［1］',\n",
       "  '消失: しょう しつ: せう－',\n",
       "  '清潔: せい けつ: ［0］',\n",
       "  '減少: げん しょう: －せう',\n",
       "  '炎: ほ の お: ほのほ',\n",
       "  '炎症: えん しょう: －しやう',\n",
       "  '熱: ねち: ▽',\n",
       "  '熱性: ねっ せい: ［0］',\n",
       "  '状態: じょう たい: じやう－',\n",
       "  '独立: どく りつ: ［0］',\n",
       "  '生活: せい かつ: －くわつ',\n",
       "  '異常: い じょう: －じやう',\n",
       "  '疾患: しっ かん: －くわん',\n",
       "  '疾病: しっ ぺい: ［0］',\n",
       "  '病: やまい: やまひ',\n",
       "  '病気: びょう き: びやう－',\n",
       "  '症: しょう: せう',\n",
       "  '症候群: しょう こうぐん: しやう－',\n",
       "  '症状: しょう じょう: しやうじやう',\n",
       "  '痛: つう: ［1］',\n",
       "  '発作: ほっ さ: ［0］',\n",
       "  '発熱: はつ ねつ: ［0］',\n",
       "  '発疹: はっ しん: ［0］',\n",
       "  '発病: はつ びょう: －びやう',\n",
       "  '発症: はっ しょう: －しやう',\n",
       "  '発育: はつ いく: ［0］',\n",
       "  '白血球: はっ けっきゅう: はくけつきう',\n",
       "  '的: [na]',\n",
       "  '皮膚: ひ ふ: ［1］',\n",
       "  '相談: そう だん: さう－',\n",
       "  '眼: がん: ［1］',\n",
       "  '破砕: は さい: ［0］',\n",
       "  '神経: しん けい: ［1］',\n",
       "  '票: ひょう: へう',\n",
       "  '程度: てい ど: ［0］',\n",
       "  '等: [na]',\n",
       "  '筋力: きん りょく: ［1］',\n",
       "  '筋肉: きん にく: ［1］',\n",
       "  '節: [na]',\n",
       "  '篤: とく: ［0］',\n",
       "  '粘膜: ねん まく: ［1］',\n",
       "  '系: けい: ［1］',\n",
       "  '紅: くれ ない: －なゐ',\n",
       "  '紫: むらさき: ［2］',\n",
       "  '紫斑: し はん: ［0］',\n",
       "  '細菌: さい きん: ［0］',\n",
       "  '総合: そう ごう: －がふ',\n",
       "  '者: [na]',\n",
       "  '聯: れん: ▼',\n",
       "  '肉芽: にく が: ［0］',\n",
       "  '肝: かん: ［1］',\n",
       "  '肝臓: かん ぞう: －ざう',\n",
       "  '肺炎: はい えん: ［0］',\n",
       "  '脊髄: せき ずい: ［2］',\n",
       "  '脳: なずき: なづき',\n",
       "  '脳炎: のう えん: なう－',\n",
       "  '脳症: のう しょう: なうしやう',\n",
       "  '腎臓: じん ぞう: －ざう',\n",
       "  '腫: しゅ: ［1］',\n",
       "  '腹痛: はら いた: ［0］',\n",
       "  '膜: まく: ［2］',\n",
       "  '薬: くすり: ［0］',\n",
       "  '蜂巣: はち す: ［0］',\n",
       "  '血小板: けっ しょうばん: －せうばん',\n",
       "  '血液: けつ えき: ［2］',\n",
       "  '血管: けっ かん: －くわん',\n",
       "  '行政: ぎょう せい: ぎやう－',\n",
       "  '衰退: すい たい: ［0］',\n",
       "  '被害: ひ がい: ［1］',\n",
       "  '見: けん: ［1］',\n",
       "  '視神経: し しんけい: ［2］',\n",
       "  '観察: かん さつ: くわん－',\n",
       "  '記入: き にゅう: －にふ',\n",
       "  '診察: しん さつ: ［0］',\n",
       "  '質: しち: ［2］',\n",
       "  '車: くるま: ［0］',\n",
       "  '軽: [na]',\n",
       "  '軽減: けい げん: ［0］',\n",
       "  '近親: きん しん: ［0］',\n",
       "  '迷走: めい そう: ［0］',\n",
       "  '通常: つう じょう: －じやう',\n",
       "  '連絡: れん らく: ［0］',\n",
       "  '週間: しゅう かん: しう－',\n",
       "  '運動: うん どう: ［0］',\n",
       "  '過去: か こ: くわ－',\n",
       "  '過敏: か びん: くわ－',\n",
       "  '適当: てき とう: －たう',\n",
       "  '部位: ぶ い: －ゐ',\n",
       "  '重: え: へ',\n",
       "  '間: あい: あひ',\n",
       "  '関節: かん せつ: くわん－',\n",
       "  '際: きわ: きは',\n",
       "  '障害: しょう がい: しやう－',\n",
       "  '非常: ひ じょう: －じやう',\n",
       "  '頭痛: ず つう: づ－',\n",
       "  '顔面: がん めん: ［0］',\n",
       "  '食事: しょく じ: ［0］',\n",
       "  '食欲: しょく よく: ［0］',\n",
       "  '飲酒: いん しゅ: ［0］',\n",
       "  '高熱: こう ねつ: かう－',\n",
       "  '鶏卵: けい らん: ［0］',\n",
       "  '鶏肉: けい にく: ［0］',\n",
       "  '麻: あさ: ［2］',\n",
       "  '黄: き: ［1］'])"
      ]
     },
     "execution_count": 561,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_jp_stress(vocab_list):\n",
    "    import re\n",
    "    import requests\n",
    "    import time\n",
    "\n",
    "    from bs4 import BeautifulSoup\n",
    "    from random import random\n",
    "    from selenium import webdriver\n",
    "\n",
    "    driver = webdriver.Chrome()\n",
    "    just_a_sec = random() \n",
    "    search_string = vocab_list\n",
    "    \n",
    "    stress_list =[]\n",
    "    stress_list_with_word =[]\n",
    "    for word in vocab_list:\n",
    "        if len(stress_list) <= 300:\n",
    "            #print(stress_list)\n",
    "            try:    \n",
    "                search_site = \"https://www.weblio.jp/content/\"\n",
    "                search_site = search_site + word\n",
    "\n",
    "                driver.get(search_site)\n",
    "                time.sleep(just_a_sec * 1)\n",
    "\n",
    "                search_box = driver.find_element_by_class_name(\"formBox\")\n",
    "\n",
    "                result_furigana = driver.find_element_by_xpath('//*[@id=\"cont\"]/div[3]/div/div[1]/h2/b')\n",
    "                result_stress = driver.find_element_by_xpath('//*[@id=\"cont\"]/div[3]/div/div[1]/h2/span')\n",
    "\n",
    "                furigana = result_furigana.text\n",
    "                word_stress = result_stress.text\n",
    "#                 if any(i.isdigit() for i in word_stress) == True:\n",
    "#                     word_stress = word_stress.replace(\"[\", \"\")\n",
    "#                     word_stress = word_stress.replace(\"]\", \"\")\n",
    "                stress_list.append(word_stress)\n",
    "                stress_list_with_word.append(word + \": \" + furigana + \": \" + word_stress)\n",
    "            except:\n",
    "                stress_list.append(\"[na]\")\n",
    "                stress_list_with_word.append(word + \": \" + \"[na]\")\n",
    "    \n",
    "    return stress_list, stress_list_with_word\n",
    " \n",
    "dic_result = get_jp_stress(list_of_words)\n",
    "dic_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>Johnson: [na]</td>\n",
       "      <td>[na]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Stevens: [na]</td>\n",
       "      <td>[na]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>じんましん: じん ましん: ［3］</td>\n",
       "      <td>［3］</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>アナフィラキシー: アナフィラキシー: ［5］</td>\n",
       "      <td>［5］</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>アレルギー: アレルギー: ［2］</td>\n",
       "      <td>［2］</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>アレルギー性: [na]</td>\n",
       "      <td>[na]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>インフルエンザ: インフルエンザ: ［5］</td>\n",
       "      <td>［5］</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>ニューロパチー: ニューロパチー: ［3］</td>\n",
       "      <td>［3］</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>ネフローゼ: ネフローゼ: ［3］</td>\n",
       "      <td>［3］</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>一: イー: ［1］</td>\n",
       "      <td>［1］</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>一方: いっ ぽう: －ぱう</td>\n",
       "      <td>－ぱう</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>一般: いっ ぱん: ［0］</td>\n",
       "      <td>［0］</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>一過: いっ か: －くわ</td>\n",
       "      <td>－くわ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>万: ばん: ［1］</td>\n",
       "      <td>［1］</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>下痢: げ り: ［0］</td>\n",
       "      <td>［0］</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>不: [na]</td>\n",
       "      <td>[na]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>両: りゃん: ［1］</td>\n",
       "      <td>［1］</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>予診: よ しん: ［0］</td>\n",
       "      <td>［0］</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>予防: よ ぼう: －ばう</td>\n",
       "      <td>－ばう</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>人: じん: ［1］</td>\n",
       "      <td>［1］</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            0\n",
       "Johnson: [na]            [na]\n",
       "Stevens: [na]            [na]\n",
       "じんましん: じん ましん: ［3］        ［3］\n",
       "アナフィラキシー: アナフィラキシー: ［5］   ［5］\n",
       "アレルギー: アレルギー: ［2］         ［2］\n",
       "アレルギー性: [na]             [na]\n",
       "インフルエンザ: インフルエンザ: ［5］     ［5］\n",
       "ニューロパチー: ニューロパチー: ［3］     ［3］\n",
       "ネフローゼ: ネフローゼ: ［3］         ［3］\n",
       "一: イー: ［1］                ［1］\n",
       "一方: いっ ぽう: －ぱう            －ぱう\n",
       "一般: いっ ぱん: ［0］            ［0］\n",
       "一過: いっ か: －くわ             －くわ\n",
       "万: ばん: ［1］                ［1］\n",
       "下痢: げ り: ［0］              ［0］\n",
       "不: [na]                  [na]\n",
       "両: りゃん: ［1］               ［1］\n",
       "予診: よ しん: ［0］             ［0］\n",
       "予防: よ ぼう: －ばう             －ばう\n",
       "人: じん: ［1］                ［1］"
      ]
     },
     "execution_count": 582,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic_result\n",
    "df = pd.DataFrame(dic_result[0], dic_result[1])\n",
    "\n",
    "df.iloc[0:20,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 583,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeEncodeError",
     "evalue": "'shift_jis' codec can't encode character '\\uff0d' in position 22: illegal multibyte sequence",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeEncodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-583-023651184a5a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mnew_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mnew_result\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"result3.csv\"\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"shift_jis\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;31m#new_result.to_csv(\"result3.csv\" , encoding=\"utf8\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mto_csv\u001b[0;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, decimal)\u001b[0m\n\u001b[1;32m   3226\u001b[0m             \u001b[0mdecimal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecimal\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3227\u001b[0m         )\n\u001b[0;32m-> 3228\u001b[0;31m         \u001b[0mformatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3230\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpath_or_buf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/io/formats/csvs.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    200\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUnicodeWriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mwriter_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/io/formats/csvs.py\u001b[0m in \u001b[0;36m_save\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    322\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_i\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_save_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_i\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/io/formats/csvs.py\u001b[0m in \u001b[0;36m_save_chunk\u001b[0;34m(self, start_i, end_i)\u001b[0m\n\u001b[1;32m    354\u001b[0m         )\n\u001b[1;32m    355\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 356\u001b[0;31m         \u001b[0mlibwriters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_csv_rows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32mpandas/_libs/writers.pyx\u001b[0m in \u001b[0;36mpandas._libs.writers.write_csv_rows\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mUnicodeEncodeError\u001b[0m: 'shift_jis' codec can't encode character '\\uff0d' in position 22: illegal multibyte sequence"
     ]
    }
   ],
   "source": [
    "#import csv\n",
    "# result.loc[:, 10] = dic_result[1]\n",
    "# result\n",
    "\n",
    "new_result = result.drop([\"test\",\"test2\",\"test3\", 1, 10], axis=1)\n",
    "new_result.insert(2, \"stress\", dic_result[1])\n",
    "new_result\n",
    "\n",
    "new_result.to_csv(\"result3.csv\" , encoding=\"shift_jis\")\n",
    "#new_result.to_csv(\"result3.csv\" , encoding=\"utf8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
