{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://qiita.com/memakura/items/20a02161fa7e18d8a693\n",
    "#https://qiita.com/y__ueda/items/7b6f2a95ea45667e1029\n",
    "\n",
    "def top_10_sites(vocab_list):\n",
    "    import re\n",
    "    import requests\n",
    "    import time\n",
    "\n",
    "    from bs4 import BeautifulSoup\n",
    "    from random import random\n",
    "    from selenium import webdriver\n",
    "\n",
    "    driver = webdriver.Chrome()\n",
    "    just_a_sec = random() \n",
    "\n",
    "    #vocab_list = [\"浅草\", \"デートスポット\", \"雑貨\"]\n",
    "    search_string = \"\"\n",
    "    for i in vocab_list:\n",
    "        search_string = search_string + i + '\"　\"'\n",
    "\n",
    "    driver.get('https://www.google.com/')\n",
    "    time.sleep(just_a_sec * 1)\n",
    "    search_box = driver.find_element_by_name(\"q\")\n",
    "    search_box.send_keys('\"' + search_string + '\"')\n",
    "    time.sleep(just_a_sec*1)\n",
    "    search_box.submit()\n",
    "    time.sleep(just_a_sec*1)\n",
    "\n",
    "    #for h1 in driver.find_elements_by_tag_name(\"h1\"):\n",
    "    #    print(h1.text)\n",
    "\n",
    "    #i = 0\n",
    "    url_list = []\n",
    "    XPATH = '//*[@id=\"rso\"]/div/div/div[\" + str(i + 1) + \"]/div/div/div[1]/a'\n",
    "    for a in driver.find_elements_by_xpath(XPATH):\n",
    "        url_list.append(a.get_attribute('href'))\n",
    "\n",
    "    for i in url_list:\n",
    "        print(i)\n",
    "    #driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://aumo.jp/articles/19756\n",
      "https://www.ozmall.co.jp/date/restaurant/cheesefondue/\n",
      "https://rtrp.jp/articles/53335/?page=3\n",
      "https://rtrp.jp/articles/8473/\n",
      "https://www.navitime.co.jp/poi?spt=01125.J000989271\n",
      "https://www.jalan.net/news/article/367303/\n",
      "https://r.gnavi.co.jp/landmark/0015543/cafe/rs/\n",
      "https://r.gnavi.co.jp/landmark/0015543/cheesefondu/rs/\n",
      "https://restaurant.ikyu.com/rsSpcl/sp/xmas/area/tokyo.htm\n",
      "https://tabelog.com/matome/tokyo/list/kwd30/CC0101/\n"
     ]
    }
   ],
   "source": [
    "word_list = [\"中野駅\", \"デートスポット\", \"チーズケーキ\"]\n",
    "\n",
    "top_10_sites(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'url_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-4c76a862a0c7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murl_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#header = r.headers[\"Content-Encoding\"]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#header\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'url_list' is not defined"
     ]
    }
   ],
   "source": [
    "url = url_list[1]\n",
    "r = requests.get(url, stream=True)\n",
    "#header = r.headers[\"Content-Encoding\"]\n",
    "#header\n",
    "\n",
    "text = r.text\n",
    "\n",
    "'''text_list = []\n",
    "for index, text in enumerate(url_list):\n",
    "    r = requests.get(url_list[index])\n",
    "    text_list.append(r.text)'''\n",
    "\n",
    "for line in text.split(\"/n\"):\n",
    "    #if \"<title>\" in line or \"<h1>\" in line:\n",
    "    if \"<p>\" in line:\n",
    "        print(line.strip())\n",
    "\n",
    "text\n",
    "#header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kanji = re.findall(\"[ぁ-龥　、　。]\", text)\n",
    "kanji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_colwidth\", 400)\n",
    "\n",
    "\n",
    "# ひらがなの抽出\n",
    "hiragana = re.findall(\"[ぁ-ん]\", text)\n",
    "# カタカナの抽出\n",
    "katakana = re.findall(\"[ァ-ン]\", text)\n",
    "# 漢字の抽出\n",
    "kanji = re.findall(\"[ぁ-龥　、　。]\", text)\n",
    "hiragana\n",
    "katakana\n",
    "joined_text = \"\".join(kanji)\n",
    "\n",
    "new_text = joined_text.split(\"。\")\n",
    "for index, line in enumerate(new_text):\n",
    "    new_text[index] += \"。\" \n",
    "\n",
    "\n",
    "for i in new_text:\n",
    "    if len(i) > 400:\n",
    "        new_text.remove(i)\n",
    "\n",
    "for i in new_text:\n",
    "    while new_text.count(i) > 1:\n",
    "        new_text.remove(i)\n",
    "        \n",
    "len(new_text)\n",
    "new_text\n",
    "df = pd.DataFrame(new_text)\n",
    "\n",
    "dfStyler = df.style.set_properties(**{'text-align': 'left'})\n",
    "dfStyler.set_table_styles([dict(selector='th', props=[('text-align', 'left')])])\n",
    "\n",
    "df\n",
    "\n",
    "df.to_csv(\"test.csv\", encoding='utf_8_sig')\n",
    "\n",
    "df\n",
    "\n",
    "new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_text\n",
    "len(new_text)\n",
    "type(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import MeCab\n",
    "import pandas as pd\n",
    "\n",
    "tagger = MeCab.Tagger(\"-Ochasen\")\n",
    "\n",
    "result_list = []\n",
    "for index, line in enumerate(new_text):\n",
    "    text = new_text[index]\n",
    "    #print(index)\n",
    "    #print(line)\n",
    "    #print(tagger.parse(line))\n",
    "    result_list.append(tagger.parse(line))\n",
    "    \n",
    "'''text = \"ブルーボトルコーヒーがオープンするなど今話題の清澄白河。\"\n",
    "result = tagger.parse(text)\n",
    "\n",
    "type(result)\n",
    "len(result)'''\n",
    "\n",
    "result_list = result.split(\"-\")\n",
    "df = pd.DataFrame(result_list)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "row = ['2', ' Marie', ' California']\n",
    "with open('people.csv', 'r') as readFile:\n",
    "    reader = csv.reader(readFile)\n",
    "    lines = list(reader)\n",
    "    lines[1] = row\n",
    "with open('people.csv', 'w') as writeFile:\n",
    "    writer = csv.writer(writeFile)\n",
    "    writer.writerows(lines)\n",
    "readFile.close()\n",
    "writeFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'header' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-660c35c52182>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavetxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"file_name.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'ブルーボトルコーヒーが'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'オープンするなど'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'今話題の清澄白河。'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\",\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfmt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'%s'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'header' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.savetxt(\"file_name.csv\", ['ブルーボトルコーヒーが','オープンするなど','今話題の清澄白河。'], delimiter=\",\", fmt='%s', header=header)\n",
    "\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "search_text1: Falseこちらに変換する文字を貼り付けて下さい。直接入力するか、ファイルからの読込み、ファイルドロップでも可能です。\n",
      "                \n",
      "search_text2: Falseこちらに変換する文字を貼り付けて下さい。直接入力するか、ファイルからの読込み、ファイルドロップでも可能です。\n",
      "                \n",
      "attrs_of_result: [['id', 'kekka'], ['name', 'ta2']]\n",
      "attrs_of_search: [['name', 'ta'], ['class', 'box0'], ['id', 'ta']]\n",
      "result_text4: \n"
     ]
    }
   ],
   "source": [
    "#OLD\n",
    "\n",
    "def pronounce(vocab_list):\n",
    "    import re\n",
    "    import requests\n",
    "    import time\n",
    "\n",
    "    from bs4 import BeautifulSoup\n",
    "    from random import random\n",
    "    from selenium import webdriver\n",
    "\n",
    "    driver = webdriver.Chrome()\n",
    "    just_a_sec = random() \n",
    "\n",
    "    search_site = \"https://www.webtoolss.com/hiragana.html\"\n",
    "\n",
    "    search_string = vocab_list\n",
    "    \n",
    "    driver.get(search_site)\n",
    "    time.sleep(just_a_sec * 1)\n",
    "    \n",
    "    #search_box = driver.find_element_by_name(\"ta\")\n",
    "    #OR\n",
    "    #search_box = driver.find_element_by_class_name(\"box0\")\n",
    "    #OR\n",
    "    search_box = driver.find_element_by_id(\"ta\")\n",
    "  \n",
    "    search_box.send_keys(search_string)\n",
    "    time.sleep(just_a_sec*1)\n",
    "\n",
    "    for i in driver.find_elements_by_class_name(\"btn\"):\n",
    "        if \"ふりがなに\" in i.text:\n",
    "            i.click()\n",
    "\n",
    "    time.sleep(just_a_sec*1)\n",
    "\n",
    "    result_box = driver.find_element_by_name(\"ta2\")\n",
    "    #OR\n",
    "    #result_box = driver.find_element_by_xpath('/html/body/div/article[1]/div[2]/textarea')\n",
    "    #OR\n",
    "    #result_box = driver.find_element_by_xpath('//*[@id=\"kekka\"]')\n",
    "    #OR\n",
    "    #result_box = driver.find_element_by_id(\"kekka\")\n",
    "    \n",
    "#    counter = 0\n",
    "#    result_list = []\n",
    "#   for i in driver.find_elements_by_id(\"kekka\"):\n",
    "#        print(counter)\n",
    "#        counter += 1\n",
    "#        result_list += result_list.append(i.text)\n",
    "\n",
    "\n",
    "    #result_box.send_keys(search_string)\n",
    "    \n",
    "    #result_box.clear()\n",
    "    #print(result_box)\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    search_text1 = search_box.get_attribute(\"textContent\")\n",
    "    search_text2 = search_box.text\n",
    "    ###search_text2 = search_box.get_attribute(\"innerText\")\n",
    "    ###search_text3 = search_box.get_attribute(\"outerText\")\n",
    "    print(\"search_text1: \" + str(search_text1 == True) + search_text1)\n",
    "    print(\"search_text2: \" + str(search_text2 == True) + search_text2)\n",
    "\n",
    "    \n",
    "    attrs_of_result = []\n",
    "    for attr in result_box.get_property('attributes'):\n",
    "        attrs_of_result.append([attr['name'], attr['value']])\n",
    "    print(\"attrs_of_result: \" + str(attrs_of_result))\n",
    "    \n",
    "    attrs_of_search = []\n",
    "    for attr in search_box.get_property('attributes'):\n",
    "        attrs_of_search.append([attr['name'], attr['value']])\n",
    "    print(\"attrs_of_search: \" + str(attrs_of_search))\n",
    "    \n",
    "    ###result_text1 = result_box.get_attribute(\"textContent\")\n",
    "    ###result_text2 = result_box.get_attribute(\"innerText\")\n",
    "    ###result_text3 = result_box.get_attribute(\"outerText\")\n",
    "    result_text4 = result_box.text\n",
    "    ###result_text5 = result_box.innerText\n",
    "    ###print(\"result_text1: \" + str(result_text1 == True) + result_text1)\n",
    "    ###print(\"result_text2: \" + str(result_text2 == True) + result_text2)\n",
    "    ###print(\"result_text3: \" + str(result_text3 == True) + result_text3)\n",
    "    print(\"result_text4: \" + result_text4)\n",
    "    ###print(\"result_text5: \" + result_text5)\n",
    "\n",
    "    \n",
    "    #return search_box.get_attribute(\"innerHTML\")\n",
    "    #OR\n",
    "    #return search_box.get_attribute(\"textContent\")\n",
    "    #return search_box.get_attribute(\"outerHTML\")\n",
    "\n",
    "sentence = \"私は台湾出身のエンジニアです。\"\n",
    "pronounce(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result_box.text: \n",
      "result_box.tag_name: textarea\n",
      "result_box.size: {'height': 200, 'width': 440}\n"
     ]
    }
   ],
   "source": [
    "def pronounce(vocab_list):\n",
    "    import re\n",
    "    import requests\n",
    "    import time\n",
    "\n",
    "    from bs4 import BeautifulSoup\n",
    "    from random import random\n",
    "    from selenium import webdriver\n",
    "\n",
    "    driver = webdriver.Chrome()\n",
    "    just_a_sec = random() \n",
    "\n",
    "    search_site = \"https://www.webtoolss.com/hiragana.html\"\n",
    "\n",
    "    search_string = vocab_list\n",
    "    \n",
    "    driver.get(search_site)\n",
    "    time.sleep(just_a_sec * 1)\n",
    "    \n",
    "    #search_box = driver.find_element_by_name(\"ta\")\n",
    "    #OR\n",
    "    #search_box = driver.find_element_by_class_name(\"box0\")\n",
    "    #OR\n",
    "    search_box = driver.find_element_by_id(\"ta\")\n",
    "  \n",
    "    search_box.send_keys(search_string)\n",
    "    time.sleep(just_a_sec*1)\n",
    "\n",
    "    for i in driver.find_elements_by_class_name(\"btn\"):\n",
    "        if \"ふりがなに\" in i.text:\n",
    "            i.click()\n",
    "\n",
    "    time.sleep(just_a_sec*1)\n",
    "\n",
    "    result_box = driver.find_element_by_name(\"ta2\")\n",
    "    #OR\n",
    "    #result_box = driver.find_element_by_xpath('/html/body/div/article[1]/div[2]/textarea')\n",
    "    #OR\n",
    "    #result_box = driver.find_element_by_xpath('//*[@id=\"kekka\"]')\n",
    "    #OR\n",
    "    #result_box = driver.find_element_by_id(\"kekka\")\n",
    "    \n",
    "#     search_text1 = search_box.get_attribute(\"textContent\")\n",
    "#     print(\"search_text1: \" + search_text1)\n",
    "#     search_text2 = search_box.get_attribute(\"outerHTML\")\n",
    "#     print(\"search_text2: \" + search_text2)\n",
    "#     search_text3 = search_box.get_attribute(\"innerHTML\")\n",
    "#     print(\"search_text3: \" + search_text3)\n",
    "#     search_text4 = search_box.text\n",
    "#     print(\"search_text4: \" + search_text4)\n",
    "    \n",
    "#     result_text1 = result_box.get_attribute(\"outerHTML\")\n",
    "#     print(\"result_text1: \" + result_text1)\n",
    "#     result_text2 = result_box.get_attribute(\"innerHTML\")\n",
    "#     print(\"result_text2: \" + result_text2)\n",
    "#     result_text3 = result_box.get_attribute(\"textContent\")\n",
    "#     print(\"result_text3: \" + result_text3)\n",
    "    result_text4 = result_box.text\n",
    "    print(\"result_box.text: \" + result_text4)\n",
    "    result_text5 = result_box.tag_name\n",
    "    print(\"result_box.tag_name: \" + result_text5)\n",
    "    result_text6 = result_box.size\n",
    "    print(\"result_box.size: \" + str(result_text6))\n",
    "\n",
    "#    attrs_of_result = []\n",
    "#    for attr in result_box.get_property('attributes'):\n",
    "#        attrs_of_result.append([attr['name'], attr['value']])\n",
    "#    print(\"attrs_of_result: \" + str(attrs_of_result))\n",
    "    \n",
    "sentence = \"検証用のコードを確認中です。\"\n",
    "pronounce(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result_furigana.text: きゅう しゅう\n",
      "result_stress.text:きうしう\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'これはけんしょうようテストです'"
      ]
     },
     "execution_count": 369,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pykakasi import kakasi\n",
    " \n",
    "text = \"これは検証用テストです\"\n",
    "\n",
    "\n",
    "kakasi = kakasi()\n",
    "# kakasi.setMode('J', 'K')\n",
    "# kakasi.setMode('J', 'a')\n",
    "kakasi.setMode('J', 'H')\n",
    "conv = kakasi.getConverter()\n",
    "#K\tカタカナ\n",
    "#H\tひらがな\n",
    "#J\t漢字\n",
    "#a\tローマ字\n",
    "#E\tわからん\n",
    " \n",
    "conv.do(text) # アオシマシュンサク"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Johnson',\n",
       " 'Stevens',\n",
       " 'じんましん',\n",
       " 'アナフィラキシー',\n",
       " 'アレルギー',\n",
       " 'アレルギー性',\n",
       " 'インフルエンザ',\n",
       " 'ニューロパチー',\n",
       " 'ネフローゼ',\n",
       " '一',\n",
       " '一方',\n",
       " '一般',\n",
       " '一過',\n",
       " '万',\n",
       " '下痢',\n",
       " '不',\n",
       " '両',\n",
       " '予診',\n",
       " '予防',\n",
       " '人',\n",
       " '今',\n",
       " '他',\n",
       " '以内',\n",
       " '低下',\n",
       " '体',\n",
       " '保健',\n",
       " '倦怠',\n",
       " '健康',\n",
       " '免疫',\n",
       " '入浴',\n",
       " '入院',\n",
       " '全身',\n",
       " '出',\n",
       " '出来',\n",
       " '分間',\n",
       " '判断',\n",
       " '前',\n",
       " '副',\n",
       " '効果',\n",
       " '動機',\n",
       " '化膿',\n",
       " '医師',\n",
       " '医療',\n",
       " '医薬品',\n",
       " '卵',\n",
       " '反応',\n",
       " '可能',\n",
       " '合併症',\n",
       " '吐',\n",
       " '呼吸',\n",
       " '嘱',\n",
       " '器',\n",
       " '困難',\n",
       " '場合',\n",
       " '多',\n",
       " '大量',\n",
       " '失神',\n",
       " '妊娠',\n",
       " '実施',\n",
       " '家族',\n",
       " '師',\n",
       " '庫',\n",
       " '当日',\n",
       " '形',\n",
       " '後',\n",
       " '徴',\n",
       " '心臓',\n",
       " '必要',\n",
       " '急',\n",
       " '急性',\n",
       " '性',\n",
       " '息',\n",
       " '悪寒',\n",
       " '意識',\n",
       " '感',\n",
       " '手足',\n",
       " '把握',\n",
       " '投与',\n",
       " '折',\n",
       " '指導',\n",
       " '指摘',\n",
       " '接種',\n",
       " '救済',\n",
       " '散在',\n",
       " '数',\n",
       " '斑',\n",
       " '方',\n",
       " '日',\n",
       " '旨',\n",
       " '期待',\n",
       " '末梢',\n",
       " '本人',\n",
       " '検査',\n",
       " '様',\n",
       " '様子',\n",
       " '機器',\n",
       " '機構',\n",
       " '機能',\n",
       " '機関',\n",
       " '次',\n",
       " '歩行',\n",
       " '歳',\n",
       " '死亡',\n",
       " '気',\n",
       " '気管支',\n",
       " '法',\n",
       " '法人',\n",
       " '注射',\n",
       " '注意',\n",
       " '浮腫',\n",
       " '消失',\n",
       " '清潔',\n",
       " '減少',\n",
       " '炎',\n",
       " '炎症',\n",
       " '熱',\n",
       " '熱性',\n",
       " '状態',\n",
       " '独立',\n",
       " '生活',\n",
       " '異常',\n",
       " '疾患',\n",
       " '疾病',\n",
       " '病',\n",
       " '病気',\n",
       " '症',\n",
       " '症候群',\n",
       " '症状',\n",
       " '痛',\n",
       " '発作',\n",
       " '発熱',\n",
       " '発疹',\n",
       " '発病',\n",
       " '発症',\n",
       " '発育',\n",
       " '白血球',\n",
       " '的',\n",
       " '皮膚',\n",
       " '相談',\n",
       " '眼',\n",
       " '破砕',\n",
       " '神経',\n",
       " '票',\n",
       " '程度',\n",
       " '等',\n",
       " '筋力',\n",
       " '筋肉',\n",
       " '節',\n",
       " '篤',\n",
       " '粘膜',\n",
       " '系',\n",
       " '紅',\n",
       " '紫',\n",
       " '紫斑',\n",
       " '細菌',\n",
       " '総合',\n",
       " '者',\n",
       " '聯',\n",
       " '肉芽',\n",
       " '肝',\n",
       " '肝臓',\n",
       " '肺炎',\n",
       " '脊髄',\n",
       " '脳',\n",
       " '脳炎',\n",
       " '脳症',\n",
       " '腎臓',\n",
       " '腫',\n",
       " '腹痛',\n",
       " '膜',\n",
       " '薬',\n",
       " '蜂巣',\n",
       " '血小板',\n",
       " '血液',\n",
       " '血管',\n",
       " '行政',\n",
       " '衰退',\n",
       " '被害',\n",
       " '見',\n",
       " '視神経',\n",
       " '観察',\n",
       " '記入',\n",
       " '診察',\n",
       " '質',\n",
       " '車',\n",
       " '軽',\n",
       " '軽減',\n",
       " '近親',\n",
       " '迷走',\n",
       " '通常',\n",
       " '連絡',\n",
       " '週間',\n",
       " '運動',\n",
       " '過去',\n",
       " '過敏',\n",
       " '適当',\n",
       " '部位',\n",
       " '重',\n",
       " '間',\n",
       " '関節',\n",
       " '際',\n",
       " '障害',\n",
       " '非常',\n",
       " '頭痛',\n",
       " '顔面',\n",
       " '食事',\n",
       " '食欲',\n",
       " '飲酒',\n",
       " '高熱',\n",
       " '鶏卵',\n",
       " '鶏肉',\n",
       " '麻',\n",
       " '黄']"
      ]
     },
     "execution_count": 420,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import MeCab\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import csv\n",
    "\n",
    "def mecab_parse(file_in, file_out = file_in + \".mecab\"):\n",
    "\n",
    "    tagger = MeCab.Tagger(\"-Ochasen\")\n",
    "    \n",
    "    with open(file_in, encoding=\"utf-8\") as input_file:\n",
    "        with open(file_out, mode=\"w\", encoding=\"utf-8\") as output_file:\n",
    "            #output_file.write(tagger.parse(input_file.read()))\n",
    "            parsed = tagger.parse(input_file.read())\n",
    "            output_file.write(parsed)\n",
    "\n",
    "    data = pd.read_table(file_out)\n",
    "    data.columns = np.arange(0,data.shape[1])\n",
    "    new_sorted_data = data[[0,1,3]].copy()\n",
    "\n",
    "    new_sorted_data.columns = [\"単語\",\"ふりがな\",\"品詞\"]\n",
    "    new_POS_series = new_sorted_data[\"品詞\"]\n",
    "\n",
    "    for index, POS in enumerate(new_POS_series):\n",
    "        word = new_POS_series[index]\n",
    "        if(type(word) == str):\n",
    "            pos_of_hyphen = word.find(\"-\")\n",
    "            new_POS_series[index] = word[:pos_of_hyphen]\n",
    "\n",
    "    new_sorted_data[\"品詞\"] = new_POS_series\n",
    "\n",
    "    new_sorted_data.loc[1, \"品詞\"]\n",
    "    row_count = new_sorted_data.shape[0]\n",
    "\n",
    "    functional_word_index = []\n",
    "    for i in np.arange(row_count):\n",
    "        pos_str =str(new_sorted_data.loc[i, \"品詞\"])\n",
    "        word_str = str(new_sorted_data.loc[i, \"単語\"])\n",
    "        \n",
    "        re_kanji = re.compile(r'^[\\u4E00-\\u9FD0]+$')\n",
    "        re_roman = re.compile(r'^[a-zA-Z]+$') #a-z:小文字、A-Z:大文字\n",
    "        re_katakana = re.compile(r'[\\u30A1-\\u30F4]+')\n",
    "        re_hiragana = re.compile(r'^[あ-ん]+$')\n",
    "\n",
    "        status_kanji = re_kanji.fullmatch(word_str)\n",
    "        \n",
    "        if pos_str in [\"助詞\", \"記号\"]:\n",
    "            functional_word_index.append(i)\n",
    "        elif status_kanji == None and len(word_str) < 5:\n",
    "            functional_word_index.append(i)\n",
    "    \n",
    "    word_table = new_sorted_data.drop(functional_word_index)\n",
    "    word_table = word_table.sort_values(by = \"単語\")\n",
    "    word_table = word_table.drop_duplicates()\n",
    "    word_table.index = np.arange(word_table.shape[0])\n",
    "    return word_table.iloc[:,:]\n",
    "\n",
    "result = mecab_parse(\"flu_detail.txt\")\n",
    "result\n",
    "\n",
    "#result.to_csv(\"result2.csv\" , encoding=\"utf-8\") #default: encoding=\"utf-8\"\n",
    "#result.to_csv(\"result2.csv\" , encoding=\"shift_jis\")\n",
    "\n",
    "vocab_list = result.iloc[:, 0]\n",
    "\n",
    "list_of_words = []\n",
    "count = 0\n",
    "for i in vocab_list:\n",
    "    list_of_words.append(i)\n",
    "\n",
    "list_of_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Johnson: [n/a]',\n",
       " 'Stevens: [n/a]',\n",
       " 'じんましん: じん ましん/［3］',\n",
       " 'アナフィラキシー: アナフィラキシー/［5］',\n",
       " 'アレルギー: アレルギー/［2］',\n",
       " 'アレルギー性: [n/a]',\n",
       " 'インフルエンザ: インフルエンザ/［5］',\n",
       " 'ニューロパチー: ニューロパチー/［3］',\n",
       " 'ネフローゼ: ネフローゼ/［3］',\n",
       " '一: イー/［1］',\n",
       " '一方: いっ ぽう/－ぱう',\n",
       " '一般: いっ ぱん/［0］',\n",
       " '一過: いっ か/－くわ',\n",
       " '万: ばん/［1］',\n",
       " '下痢: げ り/［0］',\n",
       " '不: [n/a]',\n",
       " '両: りゃん/［1］',\n",
       " '予診: よ しん/［0］',\n",
       " '予防: よ ぼう/－ばう',\n",
       " '人: じん/［1］',\n",
       " '今: [n/a]']"
      ]
     },
     "execution_count": 514,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_jp_stress(vocab_list):\n",
    "    import re\n",
    "    import requests\n",
    "    import time\n",
    "\n",
    "    from bs4 import BeautifulSoup\n",
    "    from random import random\n",
    "    from selenium import webdriver\n",
    "\n",
    "    driver = webdriver.Chrome()\n",
    "    just_a_sec = random() \n",
    "    search_string = vocab_list\n",
    "    \n",
    "    stress_list =[]\n",
    "    for word in vocab_list:\n",
    "        if len(stress_list) <= 20:\n",
    "            #print(stress_list)\n",
    "            try:    \n",
    "                search_site = \"https://www.weblio.jp/content/\"\n",
    "                search_site = search_site + word\n",
    "\n",
    "                driver.get(search_site)\n",
    "                time.sleep(just_a_sec * 1)\n",
    "\n",
    "                search_box = driver.find_element_by_class_name(\"formBox\")\n",
    "\n",
    "                result_furigana = driver.find_element_by_xpath('//*[@id=\"cont\"]/div[3]/div/div[1]/h2/b')\n",
    "                result_stress = driver.find_element_by_xpath('//*[@id=\"cont\"]/div[3]/div/div[1]/h2/span')\n",
    "\n",
    "                furigana = result_furigana.text\n",
    "                word_stress = result_stress.text\n",
    "                if any(i.isdigit() for i in word_stress) == True:\n",
    "                    word_stress = word_stress.replace(\"[\", \"\")\n",
    "                    word_stress = word_stress.replace(\"]\", \"\")\n",
    "                \n",
    "                \n",
    "                \n",
    "                stress_list.append(word + \": \" + furigana + \"/\" + word_stress)\n",
    "            except:\n",
    "                stress_list.append(word + \": \" + \"[na]\")\n",
    "    \n",
    "    return stress_list\n",
    " \n",
    "dic_result = get_jp_stress(list_of_words)\n",
    "dic_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Johnson: [n/a]',\n",
       " 'Stevens: [n/a]',\n",
       " 'じんましん: じん ましん/［3］',\n",
       " 'アナフィラキシー: アナフィラキシー/［5］',\n",
       " 'アレルギー: アレルギー/［2］',\n",
       " 'アレルギー性: [n/a]',\n",
       " 'インフルエンザ: インフルエンザ/［5］',\n",
       " 'ニューロパチー: ニューロパチー/［3］',\n",
       " 'ネフローゼ: ネフローゼ/［3］',\n",
       " '一: イー/［1］',\n",
       " '一方: いっ ぽう/－ぱう',\n",
       " '一般: いっ ぱん/［0］',\n",
       " '一過: いっ か/－くわ',\n",
       " '万: ばん/［1］',\n",
       " '下痢: げ り/［0］',\n",
       " '不: [n/a]',\n",
       " '両: りゃん/［1］',\n",
       " '予診: よ しん/［0］',\n",
       " '予防: よ ぼう/－ばう',\n",
       " '人: じん/［1］',\n",
       " '今: [n/a]']"
      ]
     },
     "execution_count": 552,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic_result\n",
    "\n",
    "for index, element in enumerate(dic_result):\n",
    "    dic_result[index].replace(\"/\", \";\")\n",
    "\n",
    "dic_result\n",
    "\n",
    "# for i in dic_result:\n",
    "#     pos_of_open_bracket = i.find(\"[\")\n",
    "#     pos_of_close_bracket = i.find(\"]\")\n",
    "    \n",
    "#     print(i )\n",
    "#     print(pos_of_open_bracket)\n",
    "#     print(pos_of_close_bracket+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>単語</th>\n",
       "      <th>ふりがな</th>\n",
       "      <th>品詞</th>\n",
       "      <th>stresse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Johnson</td>\n",
       "      <td>Johnson</td>\n",
       "      <td>名詞</td>\n",
       "      <td>Johnson[n/a]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Stevens</td>\n",
       "      <td>Stevens</td>\n",
       "      <td>名詞</td>\n",
       "      <td>Stevens[n/a]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>じんましん</td>\n",
       "      <td>ジンマシン</td>\n",
       "      <td>名詞</td>\n",
       "      <td>じんましん［3］</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>アナフィラキシー</td>\n",
       "      <td>アナフィラキシー</td>\n",
       "      <td>名詞</td>\n",
       "      <td>アナフィラキシー［5］</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>アレルギー</td>\n",
       "      <td>アレルギー</td>\n",
       "      <td>名詞</td>\n",
       "      <td>アレルギー［2］</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>208</td>\n",
       "      <td>高熱</td>\n",
       "      <td>コウネツ</td>\n",
       "      <td>名詞</td>\n",
       "      <td>高熱[n/a]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>209</td>\n",
       "      <td>鶏卵</td>\n",
       "      <td>ケイラン</td>\n",
       "      <td>名詞</td>\n",
       "      <td>鶏卵[n/a]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>鶏肉</td>\n",
       "      <td>ケイニク</td>\n",
       "      <td>名詞</td>\n",
       "      <td>鶏肉[n/a]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>211</td>\n",
       "      <td>麻</td>\n",
       "      <td>アサ</td>\n",
       "      <td>名詞</td>\n",
       "      <td>麻[n/a]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>212</td>\n",
       "      <td>黄</td>\n",
       "      <td>キ</td>\n",
       "      <td>名詞</td>\n",
       "      <td>黄[n/a]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>213 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           単語      ふりがな  品詞       stresse\n",
       "0     Johnson   Johnson  名詞  Johnson[n/a]\n",
       "1     Stevens   Stevens  名詞  Stevens[n/a]\n",
       "2       じんましん     ジンマシン  名詞      じんましん［3］\n",
       "3    アナフィラキシー  アナフィラキシー  名詞   アナフィラキシー［5］\n",
       "4       アレルギー     アレルギー  名詞      アレルギー［2］\n",
       "..        ...       ...  ..           ...\n",
       "208        高熱      コウネツ  名詞       高熱[n/a]\n",
       "209        鶏卵      ケイラン  名詞       鶏卵[n/a]\n",
       "210        鶏肉      ケイニク  名詞       鶏肉[n/a]\n",
       "211         麻        アサ  名詞        麻[n/a]\n",
       "212         黄         キ  名詞        黄[n/a]\n",
       "\n",
       "[213 rows x 4 columns]"
      ]
     },
     "execution_count": 472,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.loc[:, 10] = dic_result\n",
    "result\n",
    "\n",
    "new_result = result.drop([\"test\",\"test2\",\"test3\", 1, 10], axis=1)\n",
    "new_result.insert(3, \"stresse\", dic_result)\n",
    "new_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
