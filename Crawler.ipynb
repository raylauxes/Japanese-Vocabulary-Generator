{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://qiita.com/memakura/items/20a02161fa7e18d8a693\n",
    "#https://qiita.com/y__ueda/items/7b6f2a95ea45667e1029\n",
    "\n",
    "def top_10_sites(vocab_list):\n",
    "    import re\n",
    "    import requests\n",
    "    import time\n",
    "\n",
    "    from bs4 import BeautifulSoup\n",
    "    from random import random\n",
    "    from selenium import webdriver\n",
    "\n",
    "    driver = webdriver.Chrome()\n",
    "    just_a_sec = random() \n",
    "\n",
    "    #vocab_list = [\"浅草\", \"デートスポット\", \"雑貨\"]\n",
    "    search_string = \"\"\n",
    "    for i in vocab_list:\n",
    "        search_string = search_string + i + '\"　\"'\n",
    "\n",
    "    driver.get('https://www.google.com/')\n",
    "    time.sleep(just_a_sec * 1)\n",
    "    search_box = driver.find_element_by_name(\"q\")\n",
    "    search_box.send_keys('\"' + search_string + '\"')\n",
    "    time.sleep(just_a_sec*1)\n",
    "    search_box.submit()\n",
    "    time.sleep(just_a_sec*1)\n",
    "\n",
    "    #for h1 in driver.find_elements_by_tag_name(\"h1\"):\n",
    "    #    print(h1.text)\n",
    "\n",
    "    #i = 0\n",
    "    url_list = []\n",
    "    XPATH = '//*[@id=\"rso\"]/div/div/div[\" + str(i + 1) + \"]/div/div/div[1]/a'\n",
    "    for a in driver.find_elements_by_xpath(XPATH):\n",
    "        url_list.append(a.get_attribute('href'))\n",
    "\n",
    "    for i in url_list:\n",
    "        print(i)\n",
    "    #driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://aumo.jp/articles/19756\n",
      "https://www.ozmall.co.jp/date/restaurant/cheesefondue/\n",
      "https://rtrp.jp/articles/53335/?page=3\n",
      "https://rtrp.jp/articles/8473/\n",
      "https://www.navitime.co.jp/poi?spt=01125.J000989271\n",
      "https://www.jalan.net/news/article/367303/\n",
      "https://r.gnavi.co.jp/landmark/0015543/cafe/rs/\n",
      "https://r.gnavi.co.jp/landmark/0015543/cheesefondu/rs/\n",
      "https://restaurant.ikyu.com/rsSpcl/sp/xmas/area/tokyo.htm\n",
      "https://tabelog.com/matome/tokyo/list/kwd30/CC0101/\n"
     ]
    }
   ],
   "source": [
    "word_list = [\"中野駅\", \"デートスポット\", \"チーズケーキ\"]\n",
    "\n",
    "top_10_sites(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'url_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-4c76a862a0c7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murl_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#header = r.headers[\"Content-Encoding\"]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#header\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'url_list' is not defined"
     ]
    }
   ],
   "source": [
    "url = url_list[1]\n",
    "r = requests.get(url, stream=True)\n",
    "#header = r.headers[\"Content-Encoding\"]\n",
    "#header\n",
    "\n",
    "text = r.text\n",
    "\n",
    "'''text_list = []\n",
    "for index, text in enumerate(url_list):\n",
    "    r = requests.get(url_list[index])\n",
    "    text_list.append(r.text)'''\n",
    "\n",
    "for line in text.split(\"/n\"):\n",
    "    #if \"<title>\" in line or \"<h1>\" in line:\n",
    "    if \"<p>\" in line:\n",
    "        print(line.strip())\n",
    "\n",
    "text\n",
    "#header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kanji = re.findall(\"[ぁ-龥　、　。]\", text)\n",
    "kanji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_colwidth\", 400)\n",
    "\n",
    "\n",
    "# ひらがなの抽出\n",
    "hiragana = re.findall(\"[ぁ-ん]\", text)\n",
    "# カタカナの抽出\n",
    "katakana = re.findall(\"[ァ-ン]\", text)\n",
    "# 漢字の抽出\n",
    "kanji = re.findall(\"[ぁ-龥　、　。]\", text)\n",
    "hiragana\n",
    "katakana\n",
    "joined_text = \"\".join(kanji)\n",
    "\n",
    "new_text = joined_text.split(\"。\")\n",
    "for index, line in enumerate(new_text):\n",
    "    new_text[index] += \"。\" \n",
    "\n",
    "\n",
    "for i in new_text:\n",
    "    if len(i) > 400:\n",
    "        new_text.remove(i)\n",
    "\n",
    "for i in new_text:\n",
    "    while new_text.count(i) > 1:\n",
    "        new_text.remove(i)\n",
    "        \n",
    "len(new_text)\n",
    "new_text\n",
    "df = pd.DataFrame(new_text)\n",
    "\n",
    "dfStyler = df.style.set_properties(**{'text-align': 'left'})\n",
    "dfStyler.set_table_styles([dict(selector='th', props=[('text-align', 'left')])])\n",
    "\n",
    "df\n",
    "\n",
    "df.to_csv(\"test.csv\", encoding='utf_8_sig')\n",
    "\n",
    "df\n",
    "\n",
    "new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_text\n",
    "len(new_text)\n",
    "type(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import MeCab\n",
    "import pandas as pd\n",
    "\n",
    "tagger = MeCab.Tagger(\"-Ochasen\")\n",
    "\n",
    "result_list = []\n",
    "for index, line in enumerate(new_text):\n",
    "    text = new_text[index]\n",
    "    #print(index)\n",
    "    #print(line)\n",
    "    #print(tagger.parse(line))\n",
    "    result_list.append(tagger.parse(line))\n",
    "    \n",
    "'''text = \"ブルーボトルコーヒーがオープンするなど今話題の清澄白河。\"\n",
    "result = tagger.parse(text)\n",
    "\n",
    "type(result)\n",
    "len(result)'''\n",
    "\n",
    "result_list = result.split(\"-\")\n",
    "df = pd.DataFrame(result_list)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "row = ['2', ' Marie', ' California']\n",
    "with open('people.csv', 'r') as readFile:\n",
    "    reader = csv.reader(readFile)\n",
    "    lines = list(reader)\n",
    "    lines[1] = row\n",
    "with open('people.csv', 'w') as writeFile:\n",
    "    writer = csv.writer(writeFile)\n",
    "    writer.writerows(lines)\n",
    "readFile.close()\n",
    "writeFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'header' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-660c35c52182>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavetxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"file_name.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'ブルーボトルコーヒーが'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'オープンするなど'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'今話題の清澄白河。'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\",\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfmt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'%s'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'header' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.savetxt(\"file_name.csv\", ['ブルーボトルコーヒーが','オープンするなど','今話題の清澄白河。'], delimiter=\",\", fmt='%s', header=header)\n",
    "\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "search_text1: Falseこちらに変換する文字を貼り付けて下さい。直接入力するか、ファイルからの読込み、ファイルドロップでも可能です。\n",
      "                \n",
      "search_text2: Falseこちらに変換する文字を貼り付けて下さい。直接入力するか、ファイルからの読込み、ファイルドロップでも可能です。\n",
      "                \n",
      "attrs_of_result: [['id', 'kekka'], ['name', 'ta2']]\n",
      "attrs_of_search: [['name', 'ta'], ['class', 'box0'], ['id', 'ta']]\n",
      "result_text4: \n"
     ]
    }
   ],
   "source": [
    "#OLD\n",
    "\n",
    "def pronounce(vocab_list):\n",
    "    import re\n",
    "    import requests\n",
    "    import time\n",
    "\n",
    "    from bs4 import BeautifulSoup\n",
    "    from random import random\n",
    "    from selenium import webdriver\n",
    "\n",
    "    driver = webdriver.Chrome()\n",
    "    just_a_sec = random() \n",
    "\n",
    "    search_site = \"https://www.webtoolss.com/hiragana.html\"\n",
    "\n",
    "    search_string = vocab_list\n",
    "    \n",
    "    driver.get(search_site)\n",
    "    time.sleep(just_a_sec * 1)\n",
    "    \n",
    "    #search_box = driver.find_element_by_name(\"ta\")\n",
    "    #OR\n",
    "    #search_box = driver.find_element_by_class_name(\"box0\")\n",
    "    #OR\n",
    "    search_box = driver.find_element_by_id(\"ta\")\n",
    "  \n",
    "    search_box.send_keys(search_string)\n",
    "    time.sleep(just_a_sec*1)\n",
    "\n",
    "    for i in driver.find_elements_by_class_name(\"btn\"):\n",
    "        if \"ふりがなに\" in i.text:\n",
    "            i.click()\n",
    "\n",
    "    time.sleep(just_a_sec*1)\n",
    "\n",
    "    result_box = driver.find_element_by_name(\"ta2\")\n",
    "    #OR\n",
    "    #result_box = driver.find_element_by_xpath('/html/body/div/article[1]/div[2]/textarea')\n",
    "    #OR\n",
    "    #result_box = driver.find_element_by_xpath('//*[@id=\"kekka\"]')\n",
    "    #OR\n",
    "    #result_box = driver.find_element_by_id(\"kekka\")\n",
    "    \n",
    "#    counter = 0\n",
    "#    result_list = []\n",
    "#   for i in driver.find_elements_by_id(\"kekka\"):\n",
    "#        print(counter)\n",
    "#        counter += 1\n",
    "#        result_list += result_list.append(i.text)\n",
    "\n",
    "\n",
    "    #result_box.send_keys(search_string)\n",
    "    \n",
    "    #result_box.clear()\n",
    "    #print(result_box)\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    search_text1 = search_box.get_attribute(\"textContent\")\n",
    "    search_text2 = search_box.text\n",
    "    ###search_text2 = search_box.get_attribute(\"innerText\")\n",
    "    ###search_text3 = search_box.get_attribute(\"outerText\")\n",
    "    print(\"search_text1: \" + str(search_text1 == True) + search_text1)\n",
    "    print(\"search_text2: \" + str(search_text2 == True) + search_text2)\n",
    "\n",
    "    \n",
    "    attrs_of_result = []\n",
    "    for attr in result_box.get_property('attributes'):\n",
    "        attrs_of_result.append([attr['name'], attr['value']])\n",
    "    print(\"attrs_of_result: \" + str(attrs_of_result))\n",
    "    \n",
    "    attrs_of_search = []\n",
    "    for attr in search_box.get_property('attributes'):\n",
    "        attrs_of_search.append([attr['name'], attr['value']])\n",
    "    print(\"attrs_of_search: \" + str(attrs_of_search))\n",
    "    \n",
    "    ###result_text1 = result_box.get_attribute(\"textContent\")\n",
    "    ###result_text2 = result_box.get_attribute(\"innerText\")\n",
    "    ###result_text3 = result_box.get_attribute(\"outerText\")\n",
    "    result_text4 = result_box.text\n",
    "    ###result_text5 = result_box.innerText\n",
    "    ###print(\"result_text1: \" + str(result_text1 == True) + result_text1)\n",
    "    ###print(\"result_text2: \" + str(result_text2 == True) + result_text2)\n",
    "    ###print(\"result_text3: \" + str(result_text3 == True) + result_text3)\n",
    "    print(\"result_text4: \" + result_text4)\n",
    "    ###print(\"result_text5: \" + result_text5)\n",
    "\n",
    "    \n",
    "    #return search_box.get_attribute(\"innerHTML\")\n",
    "    #OR\n",
    "    #return search_box.get_attribute(\"textContent\")\n",
    "    #return search_box.get_attribute(\"outerHTML\")\n",
    "\n",
    "sentence = \"私は台湾出身のエンジニアです。\"\n",
    "pronounce(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result_box.text: \n",
      "result_box.tag_name: textarea\n",
      "result_box.size: {'height': 200, 'width': 440}\n"
     ]
    }
   ],
   "source": [
    "def pronounce(vocab_list):\n",
    "    import re\n",
    "    import requests\n",
    "    import time\n",
    "\n",
    "    from bs4 import BeautifulSoup\n",
    "    from random import random\n",
    "    from selenium import webdriver\n",
    "\n",
    "    driver = webdriver.Chrome()\n",
    "    just_a_sec = random() \n",
    "\n",
    "    search_site = \"https://www.webtoolss.com/hiragana.html\"\n",
    "\n",
    "    search_string = vocab_list\n",
    "    \n",
    "    driver.get(search_site)\n",
    "    time.sleep(just_a_sec * 1)\n",
    "    \n",
    "    #search_box = driver.find_element_by_name(\"ta\")\n",
    "    #OR\n",
    "    #search_box = driver.find_element_by_class_name(\"box0\")\n",
    "    #OR\n",
    "    search_box = driver.find_element_by_id(\"ta\")\n",
    "  \n",
    "    search_box.send_keys(search_string)\n",
    "    time.sleep(just_a_sec*1)\n",
    "\n",
    "    for i in driver.find_elements_by_class_name(\"btn\"):\n",
    "        if \"ふりがなに\" in i.text:\n",
    "            i.click()\n",
    "\n",
    "    time.sleep(just_a_sec*1)\n",
    "\n",
    "    result_box = driver.find_element_by_name(\"ta2\")\n",
    "    #OR\n",
    "    #result_box = driver.find_element_by_xpath('/html/body/div/article[1]/div[2]/textarea')\n",
    "    #OR\n",
    "    #result_box = driver.find_element_by_xpath('//*[@id=\"kekka\"]')\n",
    "    #OR\n",
    "    #result_box = driver.find_element_by_id(\"kekka\")\n",
    "    \n",
    "#     search_text1 = search_box.get_attribute(\"textContent\")\n",
    "#     print(\"search_text1: \" + search_text1)\n",
    "#     search_text2 = search_box.get_attribute(\"outerHTML\")\n",
    "#     print(\"search_text2: \" + search_text2)\n",
    "#     search_text3 = search_box.get_attribute(\"innerHTML\")\n",
    "#     print(\"search_text3: \" + search_text3)\n",
    "#     search_text4 = search_box.text\n",
    "#     print(\"search_text4: \" + search_text4)\n",
    "    \n",
    "#     result_text1 = result_box.get_attribute(\"outerHTML\")\n",
    "#     print(\"result_text1: \" + result_text1)\n",
    "#     result_text2 = result_box.get_attribute(\"innerHTML\")\n",
    "#     print(\"result_text2: \" + result_text2)\n",
    "#     result_text3 = result_box.get_attribute(\"textContent\")\n",
    "#     print(\"result_text3: \" + result_text3)\n",
    "    result_text4 = result_box.text\n",
    "    print(\"result_box.text: \" + result_text4)\n",
    "    result_text5 = result_box.tag_name\n",
    "    print(\"result_box.tag_name: \" + result_text5)\n",
    "    result_text6 = result_box.size\n",
    "    print(\"result_box.size: \" + str(result_text6))\n",
    "\n",
    "#    attrs_of_result = []\n",
    "#    for attr in result_box.get_property('attributes'):\n",
    "#        attrs_of_result.append([attr['name'], attr['value']])\n",
    "#    print(\"attrs_of_result: \" + str(attrs_of_result))\n",
    "    \n",
    "sentence = \"検証用のコードを確認中です。\"\n",
    "pronounce(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result_furigana.text: きゅう しゅう\n",
      "result_stress.text:きうしう\n"
     ]
    }
   ],
   "source": [
    "def get_stress(vocab_list):\n",
    "    import re\n",
    "    import requests\n",
    "    import time\n",
    "\n",
    "    from bs4 import BeautifulSoup\n",
    "    from random import random\n",
    "    from selenium import webdriver\n",
    "\n",
    "    driver = webdriver.Chrome()\n",
    "    just_a_sec = random() \n",
    "    search_string = vocab_list\n",
    "    search_site = \"https://www.weblio.jp/content/\"\n",
    "    search_site = search_site + vocab_list\n",
    "    \n",
    "    \n",
    "    driver.get(search_site)\n",
    "    time.sleep(just_a_sec * 1)\n",
    "    \n",
    "    search_box = driver.find_element_by_class_name(\"formBox\")\n",
    "\n",
    "    result_furigana = driver.find_element_by_xpath('//*[@id=\"cont\"]/div[3]/div/div[1]/h2/b')\n",
    "    result_stress = driver.find_element_by_xpath('//*[@id=\"cont\"]/div[3]/div/div[1]/h2/span')\n",
    "                                                                                  \n",
    "    result_text4 = result_furigana.text\n",
    "    print(\"result_furigana.text: \" + result_text4)\n",
    "    result_text5 = result_stress.text\n",
    "    print(\"result_stress.text:\" + result_text5)\n",
    "\n",
    "#    attrs_of_result = []\n",
    "#    for attr in result_box.get_property('attributes'):\n",
    "#        attrs_of_result.append([attr['name'], attr['value']])\n",
    "#    print(\"attrs_of_result: \" + str(attrs_of_result))\n",
    "    \n",
    "sentence = \"九州\"\n",
    "get_stress(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'あおしま   しゅんさく'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pykakasi import kakasi\n",
    " \n",
    "text = \"青島   俊作\"\n",
    "\n",
    "\n",
    "kakasi = kakasi()\n",
    "# kakasi.setMode('J', 'K')\n",
    "# kakasi.setMode('J', 'a')\n",
    "kakasi.setMode('J', 'H')\n",
    "conv = kakasi.getConverter()\n",
    " \n",
    "conv.do(text) # アオシマシュンサク"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#K\tカタカナ\n",
    "#H\tひらがな\n",
    "#J\t漢字\n",
    "#a\tローマ字\n",
    "#E\tわからん"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<ipython-input-30-02844aec8fd1>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-30-02844aec8fd1>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    with open(\"インフルエンザ\") as file_flu:\u001b[0m\n\u001b[0m                                      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "with open(\"インフルエンザ\") as file_flu:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'インフルエンザワクチンの接種についてインフルエンザの予防接種を実施するにあたって、受けられる方の健康状態をよく把握する必要があります。そのため、予診票に出来るだけ詳しくご記入ください。'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"インフルエンザワクチンの接種についてインフルエンザの予防接種を実施するにあたって、受けられる方の健康状態をよく把握する必要があります。そのため、予診票に出来るだけ詳しくご記入ください。\"\n",
    "text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-f4715e7b1914>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMeCab\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmecab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMeCab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTagger\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'-d /usr/local/lib/mecab/dic/mecab-ipadic-neologd'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# text = '解析したいテキスト'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/MeCab/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0m_mecabrc_for_bundled_dictionary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m             \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTagger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import MeCab\n",
    "\n",
    "mecab = MeCab.Tagger ('-d /usr/local/lib/mecab/dic/mecab-ipadic-neologd')\n",
    "\n",
    "# text = '解析したいテキスト'\n",
    "# mecab.parse('')#文字列がGCされるのを防ぐ\n",
    "# node = mecab.parseToNode(text)\n",
    "# while node:\n",
    "#     #単語を取得\n",
    "#     word = node.surface\n",
    "#     #品詞を取得\n",
    "#     pos = node.feature.split(\",\")[1]\n",
    "#     print('{0} , {1}'.format(word, pos))\n",
    "#     #次の単語に進める\n",
    "#     node = node.next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_with_mecab(input_file_name, output_file_name):\n",
    "    import MeCab\n",
    "    \n",
    "    tagger = MeCab.Tagger(\"-Ochasen\")\n",
    "    \n",
    "    with open(input_file_name, encoding=\"utf-8\") as input_file:\n",
    "        with open(output_file_name, mode=\"w\", encoding=\"utf-8\") as output_file:\n",
    "            output_file.write(tagger.parse(input_file.read()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_name = \"flu.txt\"\n",
    "output_data_name = input_data_name + \".mecab\"\n",
    "\n",
    "process_with_mecab(input_data_name, output_data_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "with open(\"flu.txt.mecab\") as file:\n",
    "    flu_nparray=np.array([file.read()])\n",
    "    flu_df = pd.DataFrame(flu_nparray)\n",
    "    print(type(file.read()))\n",
    "    print(file.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      名詞\n",
       "1      助詞\n",
       "2      名詞\n",
       "3      助詞\n",
       "4      名詞\n",
       "5      助詞\n",
       "6      名詞\n",
       "7      名詞\n",
       "8      助詞\n",
       "9      名詞\n",
       "10     動詞\n",
       "11     助詞\n",
       "12     記号\n",
       "13     動詞\n",
       "14     動詞\n",
       "15     名詞\n",
       "16     助詞\n",
       "17     名詞\n",
       "18     名詞\n",
       "19     助詞\n",
       "20     副詞\n",
       "21     名詞\n",
       "22     動詞\n",
       "23     名詞\n",
       "24     助詞\n",
       "25     動詞\n",
       "26     助動\n",
       "27     記号\n",
       "28     連体\n",
       "29     名詞\n",
       "30     記号\n",
       "31     名詞\n",
       "32     名詞\n",
       "33     助詞\n",
       "34     動詞\n",
       "35     助詞\n",
       "36    形容詞\n",
       "37    接頭詞\n",
       "38     名詞\n",
       "39     動詞\n",
       "40     記号\n",
       "41     名詞\n",
       "42     名詞\n",
       "43     助詞\n",
       "44     名詞\n",
       "45     助詞\n",
       "46    接頭詞\n",
       "47     名詞\n",
       "48     名詞\n",
       "49    NaN\n",
       "Name: 品詞, dtype: object"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_table('flu.txt.mecab')\n",
    "data.columns = np.arange(0,6)\n",
    "data_nparray = np.array(data)\n",
    "#print(data_nparray.shape)\n",
    "data[0]\n",
    "\n",
    "sorted_data = data[[0,1,3]]\n",
    "sorted_data.columns = [\"単語\",\"ふりがな\",\"品詞\"]\n",
    "sorted_data\n",
    "\n",
    "POS_series = sorted_data[\"品詞\"]\n",
    "new_POS_series=POS_series[:]\n",
    "\n",
    "for index, POS in enumerate(new_POS_series):\n",
    "    word = new_POS_series[index]\n",
    "    if(type(word) == str):\n",
    "        pos_of_hyphen = word.find(\"-\")\n",
    "        new_POS_series[index] = word[:pos_of_hyphen]\n",
    "\n",
    "POS_series\n",
    "new_POS_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
